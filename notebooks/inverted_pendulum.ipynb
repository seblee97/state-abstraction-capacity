{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713347c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import linalg\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gymnasium as gym\n",
    "\n",
    "# ==========================================\n",
    "# 1. COMPATIBILITY & CONFIGURATION\n",
    "# ==========================================\n",
    "# Fix for older SB3 versions referencing 'gym' instead of 'gymnasium'\n",
    "sys.modules[\"gym\"] = gym \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from huggingface_sb3 import load_from_hub\n",
    "    from stable_baselines3 import DQN, PPO\n",
    "    from stable_baselines3.common.evaluation import evaluate_policy\n",
    "except ImportError:\n",
    "    print(\"Please install requirements: pip install stable-baselines3 huggingface-sb3 shimmy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402e8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. LQR ORACLE (Fixed Physics)\n",
    "# ==========================================\n",
    "class LQRController:\n",
    "    \"\"\"\n",
    "    Computes the optimal Linear Quadratic Regulator control for CartPole.\n",
    "    Used as a 'ground truth' to cluster states by their optimal action.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        core = env.unwrapped\n",
    "        # Extract physics constants\n",
    "        self.g = core.gravity\n",
    "        self.lp = core.length\n",
    "        self.mp = core.masspole\n",
    "        self.mt = core.total_mass\n",
    "        self.K = self._calculate_gain()\n",
    "        \n",
    "    def _calculate_gain(self):\n",
    "        # Linearize dynamics around the upright fixed point\n",
    "        # Denominator accounts for the mass ratio effect on angular acceleration\n",
    "        denom = self.lp * (4.0/3 - self.mp / self.mt)\n",
    "        a = self.g / denom\n",
    "        b = -1 / denom\n",
    "        \n",
    "        # State Matrix A: [x, x_dot, theta, theta_dot] -> derivatives\n",
    "        A = np.array([\n",
    "            [0, 1, 0, 0], \n",
    "            [0, 0, 0, 0], \n",
    "            [0, 0, 0, 1], \n",
    "            [0, 0, a, 0]\n",
    "        ])\n",
    "        \n",
    "        # Control Matrix B: Force -> [x_accel, theta_accel]\n",
    "        B = np.array([[0], [1/self.mt], [0], [b/self.mt]])\n",
    "        \n",
    "        # LQR Cost Matrices\n",
    "        Q = 5 * np.eye(4) # Penalty for state deviation\n",
    "        R = np.eye(1)     # Penalty for actuation effort\n",
    "        \n",
    "        # Solve Algebraic Riccati Equation\n",
    "        P = linalg.solve_continuous_are(A, B, Q, R)\n",
    "        \n",
    "        # Compute Gain K = R^-1 B^T P\n",
    "        return np.dot(np.linalg.inv(R), np.dot(B.T, P))\n",
    "        \n",
    "    def get_optimal_action(self, state):\n",
    "        # u = -Kx\n",
    "        u = -np.dot(self.K, state)[0]\n",
    "        return 1 if u > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfe6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. LATENT EXTRACTION & SAMPLING\n",
    "# ==========================================\n",
    "def extract_representation(model, obs, target_net=\"default\"):\n",
    "    \"\"\"\n",
    "    Hooks into specific PyTorch layers to extract latent activations.\n",
    "    target_net: 'dqn_q', 'ppo_actor', or 'ppo_critic'\n",
    "    \"\"\"\n",
    "    obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(model.device)\n",
    "    \n",
    "    # 1. Select the appropriate network\n",
    "    if target_net == \"dqn_q\":\n",
    "        # DQN Q-Network: (0) Linear -> (1) ReLU\n",
    "        network = model.policy.q_net.q_net\n",
    "    elif target_net == \"ppo_actor\":\n",
    "        # PPO Policy Net: (0) Linear -> (1) Tanh\n",
    "        network = model.policy.mlp_extractor.policy_net\n",
    "    elif target_net == \"ppo_critic\":\n",
    "        # PPO Value Net: (0) Linear -> (1) Tanh\n",
    "        network = model.policy.mlp_extractor.value_net\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target_net: {target_net}\")\n",
    "\n",
    "    # 2. Forward pass through the selected network\n",
    "    with torch.no_grad():\n",
    "        out = network[2](network[1](network[0](obs_tensor)))\n",
    "    \n",
    "    return out.cpu().numpy().flatten()\n",
    "\n",
    "def collect_states(env, model, mode, n_samples):\n",
    "    \"\"\"\n",
    "    Generates a dataset of observations based on the selected mode.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        if mode == \"RANDOM\":\n",
    "            # Uniformly sample the state space\n",
    "            current_obs = env.observation_space.sample()\n",
    "            states.append(current_obs)\n",
    "            # Reset occasionally to ensure coverage, though independent sampling doesn't strictly need it\n",
    "            if _ % 50 == 0: obs, _ = env.reset()\n",
    "            \n",
    "        elif mode == \"ON_POLICY\":\n",
    "            # Follow the model's trajectory\n",
    "            states.append(obs)\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, _, term, trunc, _ = env.step(action)\n",
    "            if term or trunc:\n",
    "                obs, _ = env.reset()\n",
    "                \n",
    "    return np.array(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6d37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. METRIC CALCULATION\n",
    "# ==========================================\n",
    "def compute_cosine(v1, v2, mean=None):\n",
    "    \"\"\"Compute cosine similarity, optionally centering vectors.\"\"\"\n",
    "    v1 = v1.reshape(1, -1)\n",
    "    v2 = v2.reshape(1, -1)\n",
    "    \n",
    "    if mean is not None:\n",
    "        mean = mean.reshape(1, -1)\n",
    "        v1 = v1 - mean\n",
    "        v2 = v2 - mean\n",
    "        \n",
    "    return cosine_similarity(v1, v2)[0][0]\n",
    "\n",
    "def compute_cluster_sim(vectors, mean=None):\n",
    "    \"\"\"Average pairwise similarity within a cluster of vectors.\"\"\"\n",
    "    if len(vectors) < 2: return 0.0\n",
    "    mat = np.vstack(vectors)\n",
    "    if mean is not None:\n",
    "        mat = mat - mean\n",
    "    sim_matrix = cosine_similarity(mat)\n",
    "    # Average of upper triangle (excluding diagonal)\n",
    "    return np.mean(sim_matrix[np.triu_indices(len(mat), k=1)])\n",
    "\n",
    "def evaluate_component(model, name, target_net, env, oracle):\n",
    "    print(f\"\\n{'='*30}\\n ANALYZING: {name}\\n{'='*30}\")\n",
    "    \n",
    "    # 1. Policy Performance Check\n",
    "    mean_rew, std_rew = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "    print(f\"Policy Return: {mean_rew:.1f} +/- {std_rew:.1f}\")\n",
    "    \n",
    "    # 2. Data Collection\n",
    "    print(f\"Sampling {CONFIG['N_SAMPLES']} states ({CONFIG['SAMPLING_MODE']})...\")\n",
    "    states = collect_states(env, model, CONFIG[\"SAMPLING_MODE\"], CONFIG[\"N_SAMPLES\"])\n",
    "    \n",
    "    # 3. Compute Latents & Global Mean\n",
    "    latents = [extract_representation(model, s, target_net) for s in states]\n",
    "    global_mean = np.mean(latents, axis=0)\n",
    "    \n",
    "    # 4. Metric Loop\n",
    "    metrics = {\"sym_raw\": [], \"sym_cent\": [], \"glob_raw\": [], \"glob_cent\": []}\n",
    "    cluster_left, cluster_right = [], []\n",
    "    \n",
    "    for obs, vec in zip(states, latents):\n",
    "        # A. Mirror Symmetry: Sim(f(s), f(-s))\n",
    "        vec_mirror = extract_representation(model, -obs, target_net)\n",
    "        metrics[\"sym_raw\"].append(compute_cosine(vec, vec_mirror))\n",
    "        metrics[\"sym_cent\"].append(compute_cosine(vec, vec_mirror, global_mean))\n",
    "        \n",
    "        # B. Global Baseline: Sim(f(s), f(random))\n",
    "        # Compare current vector against a random vector from our pool\n",
    "        vec_rand = latents[np.random.randint(len(latents))]\n",
    "        metrics[\"glob_raw\"].append(compute_cosine(vec, vec_rand))\n",
    "        metrics[\"glob_cent\"].append(compute_cosine(vec, vec_rand, global_mean))\n",
    "        \n",
    "        # C. Action Clustering (LQR Oracle)\n",
    "        optimal_action = oracle.get_optimal_action(obs)\n",
    "        if optimal_action == 0:\n",
    "            cluster_left.append(vec)\n",
    "        else:\n",
    "            cluster_right.append(vec)\n",
    "            \n",
    "    # 5. Aggregate Results\n",
    "    c_raw = (compute_cluster_sim(cluster_left) + compute_cluster_sim(cluster_right)) / 2\n",
    "    c_cent = (compute_cluster_sim(cluster_left, global_mean) + compute_cluster_sim(cluster_right, global_mean)) / 2\n",
    "    \n",
    "    # 6. Formatting\n",
    "    print(f\"\\n{'-'*65}\")\n",
    "    print(f\"{'Metric':<25} | {'Uncentered':<15} | {'Centered':<15}\")\n",
    "    print(f\"{'-'*65}\")\n",
    "    print(f\"{'Global Baseline':<25} | {np.mean(metrics['glob_raw']):<15.4f} | {np.mean(metrics['glob_cent']):<15.4f}\")\n",
    "    print(f\"{'Mirror Symmetry':<25} | {np.mean(metrics['sym_raw']):<15.4f} | {np.mean(metrics['sym_cent']):<15.4f}\")\n",
    "    print(f\"{'LQR Action Cluster':<25} | {c_raw:<15.4f} | {c_cent:<15.4f}\")\n",
    "    print(f\"{'-'*65}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86afeb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "==============================\n",
      " ANALYZING: DQN (Q-Network)\n",
      "==============================\n",
      "Policy Return: 500.0 +/- 0.0\n",
      "Sampling 1000 states (ON_POLICY)...\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Metric                    | Uncentered      | Centered       \n",
      "-----------------------------------------------------------------\n",
      "Global Baseline           | 0.9865          | 0.0531         \n",
      "Mirror Symmetry           | 0.9677          | -0.5948        \n",
      "LQR Action Cluster        | 0.9936          | 0.5730         \n",
      "-----------------------------------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "==============================\n",
      " ANALYZING: PPO (Actor/Policy)\n",
      "==============================\n",
      "Policy Return: 500.0 +/- 0.0\n",
      "Sampling 1000 states (ON_POLICY)...\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Metric                    | Uncentered      | Centered       \n",
      "-----------------------------------------------------------------\n",
      "Global Baseline           | 0.2767          | 0.0282         \n",
      "Mirror Symmetry           | -0.2431         | -0.3790        \n",
      "LQR Action Cluster        | 0.6280          | 0.5564         \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "==============================\n",
      " ANALYZING: PPO (Critic/Value)\n",
      "==============================\n",
      "Policy Return: 500.0 +/- 0.0\n",
      "Sampling 1000 states (ON_POLICY)...\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Metric                    | Uncentered      | Centered       \n",
      "-----------------------------------------------------------------\n",
      "Global Baseline           | 0.9997          | 0.0518         \n",
      "Mirror Symmetry           | 0.9973          | -0.1643        \n",
      "LQR Action Cluster        | 0.9999          | 0.5488         \n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# --- USER CONFIGURATION ---\n",
    "CONFIG = {\n",
    "    \"SAMPLING_MODE\": \"ON_POLICY\",  # Options: \"ON_POLICY\" or \"RANDOM\"\n",
    "    \"N_SAMPLES\": 1000,             # Number of states to analyze\n",
    "    \"ENV_ID\": \"CartPole-v1\",\n",
    "    \"REPOS\": {\n",
    "        \"DQN\": (\"sb3/dqn-CartPole-v1\", \"dqn-CartPole-v1.zip\"),\n",
    "        \"PPO\": (\"sb3/ppo-CartPole-v1\", \"ppo-CartPole-v1.zip\")\n",
    "    }\n",
    "}\n",
    "\n",
    "env = gym.make(CONFIG[\"ENV_ID\"], render_mode=\"rgb_array\")\n",
    "oracle = LQRController(env)\n",
    "\n",
    "def load_hf_model(cls, config_key):\n",
    "    repo, filename = CONFIG[\"REPOS\"][config_key]\n",
    "    path = load_from_hub(repo_id=repo, filename=filename)\n",
    "    return cls.load(\n",
    "        path, \n",
    "        env=env, \n",
    "        custom_objects={\n",
    "            \"observation_space\": env.observation_space, \n",
    "            \"action_space\": env.action_space\n",
    "        }\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # 1. DQN Analysis\n",
    "    dqn_model = load_hf_model(DQN, \"DQN\")\n",
    "    evaluate_component(dqn_model, \"DQN (Q-Network)\", \"dqn_q\", env, oracle)\n",
    "    \n",
    "    # 2. PPO Analysis\n",
    "    ppo_model = load_hf_model(PPO, \"PPO\")\n",
    "    evaluate_component(ppo_model, \"PPO (Actor/Policy)\", \"ppo_actor\", env, oracle)\n",
    "    evaluate_component(ppo_model, \"PPO (Critic/Value)\", \"ppo_critic\", env, oracle)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff4c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "==============================\n",
      " ANALYZING: DQN (Q-Network)\n",
      "==============================\n",
      "Policy Return: 500.0 +/- 0.0\n",
      "Sampling 1000 states (RANDOM)...\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Metric                    | Uncentered      | Centered       \n",
      "-----------------------------------------------------------------\n",
      "Global Baseline           | 0.3737          | 0.0613         \n",
      "Mirror Symmetry           | 0.3687          | -0.1906        \n",
      "LQR Action Cluster        | 0.4855          | 0.1831         \n",
      "-----------------------------------------------------------------\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "\n",
      "==============================\n",
      " ANALYZING: PPO (Actor/Policy)\n",
      "==============================\n",
      "Policy Return: 500.0 +/- 0.0\n",
      "Sampling 1000 states (RANDOM)...\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Metric                    | Uncentered      | Centered       \n",
      "-----------------------------------------------------------------\n",
      "Global Baseline           | -0.0037         | -0.0032        \n",
      "Mirror Symmetry           | -0.9901         | -0.9829        \n",
      "LQR Action Cluster        | 0.3692          | 0.3686         \n",
      "-----------------------------------------------------------------\n",
      "\n",
      "==============================\n",
      " ANALYZING: PPO (Critic/Value)\n",
      "==============================\n",
      "Policy Return: 500.0 +/- 0.0\n",
      "Sampling 1000 states (RANDOM)...\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "Metric                    | Uncentered      | Centered       \n",
      "-----------------------------------------------------------------\n",
      "Global Baseline           | 0.5407          | 0.0460         \n",
      "Mirror Symmetry           | 0.1450          | -0.7759        \n",
      "LQR Action Cluster        | 0.6316          | 0.1855         \n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# --- USER CONFIGURATION ---\n",
    "CONFIG = {\n",
    "    \"SAMPLING_MODE\": \"RANDOM\",  # Options: \"ON_POLICY\" or \"RANDOM\"\n",
    "    \"N_SAMPLES\": 1000,             # Number of states to analyze\n",
    "    \"ENV_ID\": \"CartPole-v1\",\n",
    "    \"REPOS\": {\n",
    "        \"DQN\": (\"sb3/dqn-CartPole-v1\", \"dqn-CartPole-v1.zip\"),\n",
    "        \"PPO\": (\"sb3/ppo-CartPole-v1\", \"ppo-CartPole-v1.zip\")\n",
    "    }\n",
    "}\n",
    "\n",
    "env = gym.make(CONFIG[\"ENV_ID\"], render_mode=\"rgb_array\")\n",
    "oracle = LQRController(env)\n",
    "\n",
    "def load_hf_model(cls, config_key):\n",
    "    repo, filename = CONFIG[\"REPOS\"][config_key]\n",
    "    path = load_from_hub(repo_id=repo, filename=filename)\n",
    "    return cls.load(\n",
    "        path, \n",
    "        env=env, \n",
    "        custom_objects={\n",
    "            \"observation_space\": env.observation_space, \n",
    "            \"action_space\": env.action_space\n",
    "        }\n",
    "    )\n",
    "\n",
    "try:\n",
    "    # 1. DQN Analysis\n",
    "    dqn_model = load_hf_model(DQN, \"DQN\")\n",
    "    evaluate_component(dqn_model, \"DQN (Q-Network)\", \"dqn_q\", env, oracle)\n",
    "    \n",
    "    # 2. PPO Analysis\n",
    "    ppo_model = load_hf_model(PPO, \"PPO\")\n",
    "    evaluate_component(ppo_model, \"PPO (Actor/Policy)\", \"ppo_actor\", env, oracle)\n",
    "    evaluate_component(ppo_model, \"PPO (Critic/Value)\", \"ppo_critic\", env, oracle)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffe1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
