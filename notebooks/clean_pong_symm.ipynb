{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50197a28-d446-4194-986d-2e906a02a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "sys.modules['gym'] = gym\n",
    "import ale_py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from huggingface_sb3 import load_from_hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02df18f3-3484-4c49-828e-687adc55f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82187a38-5489-4336-ba52-2e0eaefa8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "PONG_RAM_INDEX = {\n",
    "    \"ball_x\":   49,\n",
    "    \"ball_y\":   54,\n",
    "    \"enemy_y\":  50,\n",
    "    \"player_y\": 51,\n",
    "}\n",
    "\n",
    "BALL_X_MIN = 50\n",
    "BALL_X_MAX = 208\n",
    "BALL_Y_MIN = 44\n",
    "BALL_Y_MAX = 207\n",
    "PLAYER_Y_MIN = 38\n",
    "PLAYER_Y_MAX = 203\n",
    "ENEMY_Y_MIN = 0\n",
    "ENEMY_Y_MAX = 208\n",
    "\n",
    "BALL_X_MID = (BALL_X_MAX - BALL_X_MIN) / 2\n",
    "BALL_Y_MID = (BALL_Y_MAX - BALL_Y_MIN) / 2\n",
    "\n",
    "PLAYER_Y_MID = (PLAYER_Y_MAX - PLAYER_Y_MIN) / 2\n",
    "ENEMY_Y_MID = (ENEMY_Y_MAX - ENEMY_Y_MIN) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7316ef21-f182-4350-8764-10793b8465f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongSymmetryAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for analyzing symmetries in Pong environments using RAM observations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Pong environment with RAM observations.\n",
    "        \n",
    "        Args:\n",
    "            render_mode: Rendering mode for the environment (None, \"human\", etc.)\n",
    "        \"\"\"\n",
    "        # Register ALE environments\n",
    "#         gym.register_envs(ale_py)\n",
    "        \n",
    "#         # Create environment with RAM observations\n",
    "#         self.env = gym.make(\n",
    "#             \"ALE/Pong-v5\",\n",
    "#             obs_type=\"ram\",\n",
    "#             render_mode=render_mode,\n",
    "#         )\n",
    "        \n",
    "        # RAM indices for extracting game state\n",
    "        self.PONG_RAM_INDEX = {\n",
    "            \"ball_x\": 49,\n",
    "            \"ball_y\": 54,\n",
    "            \"enemy_y\": 50,\n",
    "            \"player_y\": 51,\n",
    "        }\n",
    "        \n",
    "        # Game boundaries\n",
    "        self.BALL_X_MIN = 50\n",
    "        self.BALL_X_MAX = 208\n",
    "        self.BALL_Y_MIN = 44\n",
    "        self.BALL_Y_MAX = 207\n",
    "        self.PLAYER_Y_MIN = 38\n",
    "        self.PLAYER_Y_MAX = 203\n",
    "        self.ENEMY_Y_MIN = 0\n",
    "        self.ENEMY_Y_MAX = 208\n",
    "        \n",
    "        self.BALL_X_MID = (self.BALL_X_MAX - self.BALL_X_MIN) / 2\n",
    "        self.BALL_Y_MID = (self.BALL_Y_MAX - self.BALL_Y_MIN) / 2\n",
    "\n",
    "        self.PLAYER_Y_MID = (self.PLAYER_Y_MAX - self.PLAYER_Y_MIN) / 2\n",
    "        self.ENEMY_Y_MID = (self.ENEMY_Y_MAX - self.ENEMY_Y_MIN) / 2\n",
    "        \n",
    "        # Store sampled states\n",
    "        self.sampled_states: List[Dict[str, Any]] = []\n",
    "        \n",
    "    def ram_to_logic_state(\n",
    "        self, \n",
    "        ram: np.ndarray, \n",
    "        prev_state: Optional[Dict[str, Any]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert RAM observation to logical game state.\n",
    "        \n",
    "        Args:\n",
    "            ram: 128-byte RAM vector from the environment\n",
    "            prev_state: Previous logical state for velocity calculation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing ball position, velocity, and paddle positions\n",
    "        \"\"\"\n",
    "        # Extract raw positions from RAM\n",
    "        ball_x_raw = int(ram[self.PONG_RAM_INDEX[\"ball_x\"]])\n",
    "        ball_y_raw = int(ram[self.PONG_RAM_INDEX[\"ball_y\"]])\n",
    "        enemy_y_raw = int(ram[self.PONG_RAM_INDEX[\"enemy_y\"]])\n",
    "        player_y_raw = int(ram[self.PONG_RAM_INDEX[\"player_y\"]])\n",
    "        \n",
    "        # Check if ball exists (OCAtari condition)\n",
    "        ball_exists = (ball_y_raw != 0) and (ball_x_raw > 49)\n",
    "        \n",
    "        # Set ball position (None if ball doesn't exist)\n",
    "        ball_x = ball_x_raw if ball_exists else None\n",
    "        ball_y = ball_y_raw if ball_exists else None\n",
    "        \n",
    "        # Paddle positions\n",
    "        player_y = player_y_raw\n",
    "        enemy_y = enemy_y_raw\n",
    "        \n",
    "        # Calculate velocity via finite differences\n",
    "        if (prev_state is not None and \n",
    "            prev_state.get(\"ball_x\") is not None and \n",
    "            ball_x is not None):\n",
    "            ball_dx = ball_x - prev_state[\"ball_x\"]\n",
    "            ball_dy = ball_y - prev_state[\"ball_y\"]\n",
    "        else:\n",
    "            ball_dx = 0\n",
    "            ball_dy = 0\n",
    "            \n",
    "        return {\n",
    "            \"ball_x\": ball_x,\n",
    "            \"ball_y\": ball_y,\n",
    "            \"ball_dx\": ball_dx,\n",
    "            \"ball_dy\": ball_dy,\n",
    "            \"player_y\": player_y,\n",
    "            \"enemy_y\": enemy_y,\n",
    "        }\n",
    "    \n",
    "    def sample_states(\n",
    "        self, \n",
    "        num_episodes: int = 5, \n",
    "        max_steps_per_episode: int = 1000,\n",
    "        model: Optional[Any] = None,\n",
    "        max_states: Optional[int] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sample states from the environment using either a model or random policy.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of episodes to run\n",
    "            max_steps_per_episode: Maximum steps per episode\n",
    "            model: Optional model with select_action method. If None, uses random policy\n",
    "            max_states: Maximum number of states to collect. If None, no limit\n",
    "            \n",
    "        Returns:\n",
    "            List of logical game states\n",
    "        \"\"\"\n",
    "        self.sampled_states = []\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            ram, info = self.env.reset()\n",
    "            prev_state = None\n",
    "            \n",
    "            for t in range(max_steps_per_episode):\n",
    "                # Convert RAM to logical state\n",
    "                state = self.ram_to_logic_state(ram, prev_state=prev_state)\n",
    "                self.sampled_states.append(state)\n",
    "                prev_state = state\n",
    "                \n",
    "                # Check if we've reached the maximum number of states\n",
    "                if max_states is not None and len(self.sampled_states) >= max_states:\n",
    "                    return self.sampled_states\n",
    "                \n",
    "                # Select action using model or random policy\n",
    "                if model is not None:\n",
    "                    # Assume model has a select_action method\n",
    "                    if hasattr(model, 'select_action'):\n",
    "                        action = model.select_action(ram)\n",
    "                    elif hasattr(model, 'predict'):\n",
    "                        action = model.predict(ram)\n",
    "                    else:\n",
    "                        # Try calling the model directly\n",
    "                        action = model(ram)\n",
    "                else:\n",
    "                    # Random policy\n",
    "                    action = self.env.action_space.sample()\n",
    "                \n",
    "                # Take step in environment\n",
    "                ram, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "        return self.sampled_states\n",
    "    \n",
    "    def naive_symmetry(self, state_1: Dict[str, Any], state_2: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if two states are symmetric under naive symmetry assumption.\n",
    "        Ignores scores and opponent paddle position, bins coordinates.\n",
    "        \n",
    "        Args:\n",
    "            state_1: First game state\n",
    "            state_2: Second game state\n",
    "            \n",
    "        Returns:\n",
    "            True if states are considered symmetric, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if ball x positions and x velocities match\n",
    "        ballx = state_1[\"ball_x\"] == state_2[\"ball_x\"]\n",
    "        balldx = state_1[\"ball_dx\"] == state_2[\"ball_dx\"]\n",
    "        \n",
    "        if not ballx or not balldx:\n",
    "            return False\n",
    "            \n",
    "        bally = state_1[\"ball_y\"] == state_2[\"ball_y\"]\n",
    "        balldy = state_1[\"ball_dy\"] == state_2[\"ball_dy\"]\n",
    "        playery = state_1[\"player_y\"] == state_2[\"player_y\"]\n",
    "        \n",
    "        # Case 1: Completely equal\n",
    "        if bally and balldy and playery:\n",
    "            return True\n",
    "        \n",
    "        # Case 2: Ball off screen, paddle positions symmetric\n",
    "        if state_1[\"ball_y\"] is None and state_2[\"ball_y\"] is None:\n",
    "            if (abs(state_1[\"player_y\"] - self.BALL_Y_MID) == \n",
    "                abs(state_2[\"player_y\"] - self.BALL_Y_MID)):\n",
    "                return True\n",
    "        \n",
    "        # Case 3: Symmetric reflection about y-axis\n",
    "        if (state_1[\"ball_y\"] is not None and state_2[\"ball_y\"] is not None):\n",
    "            ball_y_symmetric = (abs(state_1[\"ball_y\"] - self.BALL_Y_MID) == \n",
    "                              abs(state_2[\"ball_y\"] - self.BALL_Y_MID))\n",
    "            player_y_symmetric = (abs(state_1[\"player_y\"] - self.PLAYER_Y_MID) == \n",
    "                                abs(state_2[\"player_y\"] - self.PLAYER_Y_MID))\n",
    "            ball_dy_opposite = state_1[\"ball_dy\"] == -state_2[\"ball_dy\"]\n",
    "            \n",
    "            if ball_y_symmetric and player_y_symmetric and ball_dy_opposite:\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def generate_similarity_matrix(\n",
    "        self, \n",
    "        states: Optional[List[Dict[str, Any]]] = None,\n",
    "        symmetry_function: Optional[Callable] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a similarity matrix for the given states using a symmetry function.\n",
    "        \n",
    "        Args:\n",
    "            states: List of states to compare. If None, uses self.sampled_states\n",
    "            symmetry_function: Function to check symmetry between two states.\n",
    "                             If None, uses self.naive_symmetry\n",
    "                             \n",
    "        Returns:\n",
    "            Binary similarity matrix where entry (i,j) is 1 if states i and j are symmetric\n",
    "        \"\"\"\n",
    "        if states is None:\n",
    "            states = self.sampled_states\n",
    "            \n",
    "        if symmetry_function is None:\n",
    "            symmetry_function = self.naive_symmetry\n",
    "            \n",
    "        if len(states) == 0:\n",
    "            raise ValueError(\"No states provided for similarity matrix generation\")\n",
    "            \n",
    "        matrix = np.zeros((len(states), len(states)), dtype=int)\n",
    "        \n",
    "        for i, state_i in enumerate(states):\n",
    "            for j, state_j in enumerate(states):\n",
    "                matrix[i][j] = int(symmetry_function(state_i, state_j))\n",
    "                \n",
    "        return matrix\n",
    "    \n",
    "    def get_similarity_stats(self, similarity_matrix: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute statistics about the similarity matrix.\n",
    "        \n",
    "        Args:\n",
    "            similarity_matrix: Binary similarity matrix\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with statistics about symmetries\n",
    "        \"\"\"\n",
    "        n = similarity_matrix.shape[0]\n",
    "        total_pairs = n * (n - 1) // 2  # Exclude diagonal\n",
    "        \n",
    "        # Count symmetric pairs (excluding diagonal)\n",
    "        symmetric_pairs = (np.sum(similarity_matrix) - n) // 2\n",
    "        \n",
    "        return {\n",
    "            \"total_states\": n,\n",
    "            \"total_pairs\": total_pairs,\n",
    "            \"symmetric_pairs\": symmetric_pairs,\n",
    "            \"symmetry_ratio\": symmetric_pairs / total_pairs if total_pairs > 0 else 0.0,\n",
    "            \"diagonal_sum\": np.sum(np.diag(similarity_matrix)),\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        # self.env.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c9c7d3-67f9-4be2-88d9-653a5bee4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_symmetry(state_1, state_2):\n",
    "    \"\"\"\n",
    "    In this naive treatment, I ignore: scores, opponent paddle position.\n",
    "    I bin the x, y coordinates of the ball.\n",
    "    I bin the y coordinate of the player paddle.\n",
    "    Function returns True if: \n",
    "        - coarse x is equal in both states\n",
    "        - dx is equal in both states\n",
    "        - AND ball y and paddle y are equal OR equal on reflection in y=0 line\n",
    "    \"\"\"\n",
    "    ballx = state_1[\"ball_x\"] == state_2[\"ball_x\"]\n",
    "    balldx = state_1[\"ball_dx\"] == state_2[\"ball_dx\"]\n",
    "\n",
    "    if not ballx or not balldx:\n",
    "        return False\n",
    "        \n",
    "    bally = state_1[\"ball_y\"] == state_2[\"ball_y\"]\n",
    "    balldy = state_1[\"ball_dy\"] == state_2[\"ball_dy\"] \n",
    "\n",
    "    playery = state_1[\"player_y\"] == state_2[\"player_y\"] \n",
    "\n",
    "    # case 1: completely equal\n",
    "    # if bally:\n",
    "    #     if balldy:\n",
    "    #         if playery:\n",
    "    #             return True\n",
    "\n",
    "    # case 2: ball off screen. paddle flipped\n",
    "    if state_1[\"ball_y\"] is None:\n",
    "        if abs(state_1[\"player_y\"] - BALL_Y_MID) == abs(state_2[\"player_y\"] - BALL_Y_MID):\n",
    "            return True\n",
    "        \n",
    "    # case 2: flipped\n",
    "    if state_1[\"ball_y\"] is not None:\n",
    "        if abs(state_1[\"ball_y\"] - BALL_Y_MID) == abs(state_2[\"ball_y\"] - BALL_Y_MID):\n",
    "            if abs(state_1[\"player_y\"] - BALL_Y_MID) == abs(state_2[\"player_y\"] - BALL_Y_MID):\n",
    "                if state_1[\"ball_dy\"] == -state_2[\"ball_dy\"]:\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ac1ae4-383c-4416-b1e8-743b070efb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ram_to_logic_state(\n",
    "    ram: np.ndarray,\n",
    "    prev_state: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Minimal logic-level state for ALE Pong from a 128-byte RAM vector.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"ball_x\":   int or None,\n",
    "            \"ball_y\":   int or None,\n",
    "            \"ball_dx\":  int,\n",
    "            \"ball_dy\":  int,\n",
    "            \"player_y\": int or None,\n",
    "            \"enemy_y\":  int or None,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Raw positions from RAM (as in OCAtari)\n",
    "    ball_x_raw   = int(ram[PONG_RAM_INDEX[\"ball_x\"]])\n",
    "    ball_y_raw   = int(ram[PONG_RAM_INDEX[\"ball_y\"]])\n",
    "    enemy_y_raw  = int(ram[PONG_RAM_INDEX[\"enemy_y\"]])\n",
    "    player_y_raw = int(ram[PONG_RAM_INDEX[\"player_y\"]])\n",
    "\n",
    "    # OCAtari condition for “ball exists”\n",
    "    ball_exists = (ball_y_raw != 0) and (ball_x_raw > 49)\n",
    "\n",
    "    # If you want to treat \"no ball\" explicitly:\n",
    "    ball_x = ball_x_raw if ball_exists else None\n",
    "    ball_y = ball_y_raw if ball_exists else None\n",
    "\n",
    "    # Paddles basically always exist when game is running;\n",
    "    # if you want to mirror OCAtari, you could add checks similar to ram[50] / ram[51] ranges.\n",
    "    player_y = player_y_raw\n",
    "    enemy_y  = enemy_y_raw\n",
    "\n",
    "    # Velocity via finite differences\n",
    "    if prev_state is not None and prev_state.get(\"ball_x\") is not None and ball_x is not None:\n",
    "        ball_dx = ball_x - prev_state[\"ball_x\"]\n",
    "        ball_dy = ball_y - prev_state[\"ball_y\"]\n",
    "    else:\n",
    "        ball_dx = 0\n",
    "        ball_dy = 0\n",
    "\n",
    "    return {\n",
    "        \"ball_x\":   ball_x,\n",
    "        \"ball_y\":   ball_y,\n",
    "        \"ball_dx\":  ball_dx,\n",
    "        \"ball_dy\":  ball_dy,\n",
    "        \"player_y\": player_y,\n",
    "        \"enemy_y\":  enemy_y,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e167bbc1-97fe-4523-b3ed-350c9b095dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelActivationExtractor:\n",
    "    \"\"\"\n",
    "    Extract activations from different layers of trained RL models.\n",
    "    Handles various model architectures robustly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_type='ppo'):\n",
    "        self.model = model\n",
    "        self.model_type = model_type.lower()\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self, layer_names=None):\n",
    "        \"\"\"Register forward hooks to capture activations with robust architecture handling.\"\"\"\n",
    "        def hook_fn(name):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    self.activations[name] = output.detach().cpu().numpy()\n",
    "                elif isinstance(output, (list, tuple)) and len(output) > 0:\n",
    "                    # Handle cases where output is a tuple/list\n",
    "                    if isinstance(output[0], torch.Tensor):\n",
    "                        self.activations[name] = output[0].detach().cpu().numpy()\n",
    "            return hook\n",
    "            \n",
    "        # Get the policy/q-network\n",
    "        if hasattr(self.model, 'policy'):\n",
    "            net = self.model.policy\n",
    "            net_type = 'policy'\n",
    "        elif hasattr(self.model, 'q_net'):\n",
    "            net = self.model.q_net\n",
    "            net_type = 'q_net'\n",
    "        else:\n",
    "            net = self.model\n",
    "            net_type = 'unknown'\n",
    "            \n",
    "        print(f\"Network type: {net_type}, Architecture: {type(net)}\")\n",
    "        \n",
    "        # Handle different network architectures\n",
    "        feature_extractors_found = []\n",
    "        \n",
    "        # Method 1: Handle ActorCriticCnnPolicy (PPO/A2C) - separate feature extractors\n",
    "        if hasattr(net, 'pi_features_extractor') and net.pi_features_extractor is not None:\n",
    "            self._register_cnn_hooks(net.pi_features_extractor, 'policy_features', hook_fn, layer_names)\n",
    "            feature_extractors_found.append('policy_features')\n",
    "            \n",
    "        if hasattr(net, 'vf_features_extractor') and net.vf_features_extractor is not None:\n",
    "            self._register_cnn_hooks(net.vf_features_extractor, 'value_features', hook_fn, layer_names)\n",
    "            feature_extractors_found.append('value_features')\n",
    "        \n",
    "        # Method 2: Handle shared features_extractor (PPO/A2C/DQN)\n",
    "        if hasattr(net, 'features_extractor') and net.features_extractor is not None:\n",
    "            self._register_cnn_hooks(net.features_extractor, 'shared_features', hook_fn, layer_names)\n",
    "            feature_extractors_found.append('shared_features')\n",
    "        \n",
    "        # Method 3: Handle DQN QNetwork structure\n",
    "        if self.model_type in ['dqn', 'qrdqn']:\n",
    "            if hasattr(self.model, 'q_net') and self.model.q_net is not None:\n",
    "                q_net = self.model.q_net\n",
    "                \n",
    "                # Check if q_net has features_extractor (DQN structure)\n",
    "                if hasattr(q_net, 'features_extractor') and q_net.features_extractor is not None:\n",
    "                    self._register_cnn_hooks(q_net.features_extractor, 'q_features', hook_fn, layer_names)\n",
    "                    feature_extractors_found.append('q_features')\n",
    "                \n",
    "                # Register hooks for q_net layers (final Q-value layers)\n",
    "                if hasattr(q_net, 'q_net') and q_net.q_net is not None:\n",
    "                    self._register_final_layers(q_net.q_net, 'q_net', hook_fn, layer_names)\n",
    "                elif isinstance(q_net, nn.Sequential):\n",
    "                    self._register_final_layers(q_net, 'q_net', hook_fn, layer_names)\n",
    "        \n",
    "        if not feature_extractors_found:\n",
    "            print(\"Warning: Could not find any feature extractors in model architecture\")\n",
    "            print(\"Model structure:\")\n",
    "            print(net)\n",
    "            # Try to auto-discover layers\n",
    "            self._auto_discover_layers(net, hook_fn, layer_names)\n",
    "            \n",
    "        # Register hooks for final layers (action, value, q-networks)\n",
    "        self._register_final_layer_hooks(net, hook_fn, layer_names)\n",
    "        \n",
    "        print(f\"Total hooks registered: {len(self.hooks)}\")\n",
    "        print(f\"Feature extractors found: {feature_extractors_found}\")\n",
    "        \n",
    "    def _register_cnn_hooks(self, feature_extractor, prefix, hook_fn, layer_names):\n",
    "        \"\"\"Register hooks for a CNN feature extractor.\"\"\"\n",
    "        # Handle NatureCNN structure (both DQN and PPO use this)\n",
    "        if hasattr(feature_extractor, 'cnn'):\n",
    "            cnn_layers = feature_extractor.cnn\n",
    "            print(f\"Found CNN layers in {prefix} with {len(cnn_layers)} layers\")\n",
    "            \n",
    "            for i, layer in enumerate(cnn_layers):\n",
    "                if isinstance(layer, (nn.Conv2d, nn.Linear, nn.ReLU, nn.MaxPool2d, nn.Flatten)):\n",
    "                    name = f'{prefix}_cnn_{i}_{layer.__class__.__name__}'\n",
    "                    if layer_names is None or name in layer_names:\n",
    "                        hook = layer.register_forward_hook(hook_fn(name))\n",
    "                        self.hooks.append(hook)\n",
    "                        print(f\"  Registered hook for {name}\")\n",
    "        \n",
    "        # Handle linear layers in feature extractor\n",
    "        if hasattr(feature_extractor, 'linear'):\n",
    "            linear_layers = feature_extractor.linear\n",
    "            print(f\"Found linear layers in {prefix} with {len(linear_layers)} layers\")\n",
    "            \n",
    "            for i, layer in enumerate(linear_layers):\n",
    "                if isinstance(layer, (nn.Linear, nn.ReLU)):\n",
    "                    name = f'{prefix}_linear_{i}_{layer.__class__.__name__}'\n",
    "                    if layer_names is None or name in layer_names:\n",
    "                        hook = layer.register_forward_hook(hook_fn(name))\n",
    "                        self.hooks.append(hook)\n",
    "                        print(f\"  Registered hook for {name}\")\n",
    "    \n",
    "    def _register_final_layers(self, layer_sequence, prefix, hook_fn, layer_names):\n",
    "        \"\"\"Register hooks for final layers (q_net, action_net, value_net).\"\"\"\n",
    "        if isinstance(layer_sequence, nn.Sequential):\n",
    "            for i, layer in enumerate(layer_sequence):\n",
    "                if isinstance(layer, (nn.Linear, nn.ReLU)):\n",
    "                    name = f'{prefix}_{i}_{layer.__class__.__name__}'\n",
    "                    if layer_names is None or name in layer_names:\n",
    "                        hook = layer.register_forward_hook(hook_fn(name))\n",
    "                        self.hooks.append(hook)\n",
    "                        print(f\"  Registered hook for {name}\")\n",
    "        elif isinstance(layer_sequence, (nn.Linear, nn.ReLU)):\n",
    "            # Single layer\n",
    "            name = f'{prefix}_{layer_sequence.__class__.__name__}'\n",
    "            if layer_names is None or name in layer_names:\n",
    "                hook = layer_sequence.register_forward_hook(hook_fn(name))\n",
    "                self.hooks.append(hook)\n",
    "                print(f\"  Registered hook for {name}\")\n",
    "    \n",
    "    def _register_final_layer_hooks(self, net, hook_fn, layer_names):\n",
    "        \"\"\"Register hooks for final action and value layers.\"\"\"\n",
    "        # Action network (policy head)\n",
    "        if hasattr(net, 'action_net') and net.action_net is not None:\n",
    "            name = f'action_net_{net.action_net.__class__.__name__}'\n",
    "            if layer_names is None or name in layer_names:\n",
    "                hook = net.action_net.register_forward_hook(hook_fn(name))\n",
    "                self.hooks.append(hook)\n",
    "                print(f\"  Registered hook for {name}\")\n",
    "                \n",
    "        # Value network (critic head)\n",
    "        if hasattr(net, 'value_net') and net.value_net is not None:\n",
    "            name = f'value_net_{net.value_net.__class__.__name__}'\n",
    "            if layer_names is None or name in layer_names:\n",
    "                hook = net.value_net.register_forward_hook(hook_fn(name))\n",
    "                self.hooks.append(hook)\n",
    "                print(f\"  Registered hook for {name}\")\n",
    "        \n",
    "        # Q-network (DQN head) - check if it's at the top level\n",
    "        if hasattr(net, 'q_net') and net.q_net is not None and not hasattr(net, 'features_extractor'):\n",
    "            # This is likely the final q_net layer\n",
    "            self._register_final_layers(net.q_net, 'q_output', hook_fn, layer_names)\n",
    "        \n",
    "        # For PPO/A2C: look for mlp_extractor (though this one is empty in your case)\n",
    "        if hasattr(net, 'mlp_extractor') and net.mlp_extractor is not None:\n",
    "            mlp = net.mlp_extractor\n",
    "            \n",
    "            # Policy MLP\n",
    "            if hasattr(mlp, 'policy_net') and mlp.policy_net is not None and len(mlp.policy_net) > 0:\n",
    "                for i, layer in enumerate(mlp.policy_net):\n",
    "                    if isinstance(layer, (nn.Linear, nn.ReLU)):\n",
    "                        name = f'mlp_policy_{i}_{layer.__class__.__name__}'\n",
    "                        if layer_names is None or name in layer_names:\n",
    "                            hook = layer.register_forward_hook(hook_fn(name))\n",
    "                            self.hooks.append(hook)\n",
    "                            print(f\"  Registered hook for {name}\")\n",
    "            \n",
    "            # Value MLP  \n",
    "            if hasattr(mlp, 'value_net') and mlp.value_net is not None and len(mlp.value_net) > 0:\n",
    "                for i, layer in enumerate(mlp.value_net):\n",
    "                    if isinstance(layer, (nn.Linear, nn.ReLU)):\n",
    "                        name = f'mlp_value_{i}_{layer.__class__.__name__}'\n",
    "                        if layer_names is None or name in layer_names:\n",
    "                            hook = layer.register_forward_hook(hook_fn(name))\n",
    "                            self.hooks.append(hook)\n",
    "                            print(f\"  Registered hook for {name}\")\n",
    "    \n",
    "    def _auto_discover_layers(self, net, hook_fn, layer_names):\n",
    "        \"\"\"Auto-discover layers if standard patterns fail.\"\"\"\n",
    "        print(\"Attempting auto-discovery of layers...\")\n",
    "        \n",
    "        def register_if_target_layer(module, name):\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear, nn.ReLU, nn.MaxPool2d, nn.Flatten)):\n",
    "                hook_name = f'auto_{name}_{module.__class__.__name__}'\n",
    "                if layer_names is None or hook_name in layer_names:\n",
    "                    hook = module.register_forward_hook(hook_fn(hook_name))\n",
    "                    self.hooks.append(hook)\n",
    "                    print(f\"  Auto-discovered: {hook_name}\")\n",
    "        \n",
    "        # Walk through all named modules\n",
    "        for name, module in net.named_modules():\n",
    "            if name:  # Skip the root module (empty name)\n",
    "                register_if_target_layer(module, name.replace('.', '_'))\n",
    "    \n",
    "    def extract_activations(self, states):\n",
    "        \"\"\"Extract activations for a batch of states.\"\"\"\n",
    "        self.activations.clear()\n",
    "        \n",
    "        # Convert states to tensor\n",
    "        if isinstance(states, np.ndarray):\n",
    "            states_tensor = torch.FloatTensor(states)\n",
    "        else:\n",
    "            states_tensor = torch.FloatTensor(np.array(states))\n",
    "            \n",
    "        # Ensure correct shape (batch_size, channels, height, width)\n",
    "        if len(states_tensor.shape) == 3:\n",
    "            states_tensor = states_tensor.unsqueeze(0)\n",
    "        if states_tensor.shape[-1] == 3:  # If channels last\n",
    "            states_tensor = states_tensor.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Normalize to [0, 1] if needed\n",
    "        if states_tensor.max() > 1.0:\n",
    "            states_tensor = states_tensor / 255.0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                if self.model_type in ['ppo', 'a2c']:\n",
    "                    # For policy gradient methods - this will trigger both policy and value feature extractors\n",
    "                    self.model.policy(states_tensor)\n",
    "                elif self.model_type in ['dqn', 'qrdqn']:\n",
    "                    # For Q-learning methods\n",
    "                    if hasattr(self.model, 'q_net'):\n",
    "                        self.model.q_net(states_tensor)\n",
    "                    else:\n",
    "                        # Fallback\n",
    "                        self.model.policy(states_tensor)\n",
    "                else:\n",
    "                    # Generic fallback\n",
    "                    if hasattr(self.model, 'policy'):\n",
    "                        self.model.policy(states_tensor)\n",
    "                    elif hasattr(self.model, 'q_net'):\n",
    "                        self.model.q_net(states_tensor)\n",
    "                    else:\n",
    "                        self.model(states_tensor)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during forward pass: {e}\")\n",
    "                return {}\n",
    "                \n",
    "        return dict(self.activations)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()\n",
    "        self.activations.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d789c43-8fff-49ec-ae95-3a25d99dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_models(env_name: str = \"PongNoFrameskip-v4\"):\n",
    "    \"\"\"Download pre-trained models from Hugging Face.\"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # pre-create environment\n",
    "    # env = gym.make(env_name, render_mode=\"human\")\n",
    "    # env = ChannelFirstWrapper(env)\n",
    "    env = make_atari_env(env_name, n_envs=1, env_kwargs={\"render_mode\": \"human\"})\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    env = VecTransposeImage(env)\n",
    "\n",
    "    # 4. Load with CUSTOM_OBJECTS override\n",
    "    # This replaces the metadata from the file with the spaces from your current env\n",
    "    custom_objects = {\n",
    "        \"observation_space\": env.observation_space,\n",
    "        \"action_space\": env.action_space\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # PPO model\n",
    "        print(\"Downloading PPO model...\")\n",
    "        ppo_path = load_from_hub(\n",
    "            repo_id=f\"sb3/ppo-{env_name}\",\n",
    "            filename=f\"ppo-{env_name}.zip\"\n",
    "        )\n",
    "        ppo_model = PPO.load(ppo_path, env=env, custom_objects=custom_objects)\n",
    "        models['ppo'] = ppo_model\n",
    "        print(\"PPO model downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download PPO model: {e}\")\n",
    "\n",
    "    # Try DQN with multiple strategies\n",
    "    print(\"Attempting DQN download with multiple fallback strategies...\")\n",
    "    dqn_success = False\n",
    "    \n",
    "    # First get the file\n",
    "    try:\n",
    "        dqn_path = load_from_hub(\n",
    "            repo_id=\"sb3/dqn-PongNoFrameskip-v4\",\n",
    "            filename=\"dqn-PongNoFrameskip-v4.zip\"\n",
    "        )\n",
    "        print(\"  DQN file downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download DQN file: {e}\")\n",
    "        dqn_path = None\n",
    "\n",
    "    if dqn_path:\n",
    "        # Strategy 1: Override replay buffer settings\n",
    "        if not dqn_success:\n",
    "            try:\n",
    "                print(\"  Strategy 1: Override replay buffer settings...\")\n",
    "                dqn_custom_objects = {\n",
    "                    \"observation_space\": env.observation_space,\n",
    "                    \"action_space\": env.action_space,\n",
    "                    # Force compatible replay buffer settings\n",
    "                    \"optimize_memory_usage\": False,\n",
    "                    \"handle_timeout_termination\": False,\n",
    "                }\n",
    "                dqn_model = DQN.load(dqn_path, env=env, custom_objects=dqn_custom_objects)\n",
    "                models['dqn'] = dqn_model\n",
    "                print(\"DQN model downloaded successfully (strategy 1)\")\n",
    "                dqn_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Strategy 1 failed: {e}\")\n",
    "\n",
    "        # Strategy 2: Load without environment, then set it\n",
    "        if not dqn_success:\n",
    "            try:\n",
    "                print(\"  Strategy 2: Load without environment...\")\n",
    "                dqn_custom_objects = {\n",
    "                    \"observation_space\": env.observation_space,\n",
    "                    \"action_space\": env.action_space,\n",
    "                }\n",
    "                dqn_model = DQN.load(dqn_path, env=None, custom_objects=dqn_custom_objects)\n",
    "                # Create a fresh replay buffer with compatible settings\n",
    "                dqn_model.set_env(env)\n",
    "                models['dqn'] = dqn_model\n",
    "                print(\"DQN model downloaded successfully (strategy 2)\")\n",
    "                dqn_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Strategy 2 failed: {e}\")\n",
    "\n",
    "        # Strategy 3: Modify the model after loading\n",
    "        if not dqn_success:\n",
    "            try:\n",
    "                print(\"  Strategy 3: Load and modify replay buffer...\")\n",
    "                # Load with minimal custom objects\n",
    "                dqn_model = DQN.load(dqn_path, env=None)\n",
    "                # Manually fix the replay buffer settings\n",
    "                if hasattr(dqn_model, 'replay_buffer') and dqn_model.replay_buffer is not None:\n",
    "                    dqn_model.replay_buffer.optimize_memory_usage = False\n",
    "                    dqn_model.replay_buffer.handle_timeout_termination = False\n",
    "                dqn_model.set_env(env)\n",
    "                models['dqn'] = dqn_model\n",
    "                print(\"DQN model downloaded successfully (strategy 3)\")\n",
    "                dqn_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Strategy 3 failed: {e}\")\n",
    "\n",
    "    if not dqn_success:\n",
    "        print(\"All DQN strategies failed. Continuing with PPO and A2C only.\")\n",
    "\n",
    "    try:\n",
    "        # A2C model\n",
    "        print(\"Downloading A2C model...\")\n",
    "        a2c_path = load_from_hub(\n",
    "            repo_id=f\"sb3/a2c-{env_name}\",\n",
    "            filename=f\"a2c-{env_name}.zip\"\n",
    "        )\n",
    "        a2c_model = A2C.load(a2c_path, env=env, custom_objects=custom_objects)\n",
    "        models['a2c'] = a2c_model\n",
    "        print(\"A2C model downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download A2C model: {e}\")\n",
    "\n",
    "    # Try QRDQN (Quantile Regression DQN)\n",
    "    print(\"Attempting QRDQN download...\")\n",
    "    qrdqn_success = False\n",
    "    \n",
    "    # Try different possible repository names for QRDQN\n",
    "    qrdqn_repos = [\n",
    "        (\"sb3/qrdqn-PongNoFrameskip-v4\", \"qrdqn-PongNoFrameskip-v4.zip\"),\n",
    "        (\"sb3-contrib/qrdqn-PongNoFrameskip-v4\", \"qrdqn-PongNoFrameskip-v4.zip\"),\n",
    "        (\"sb3/qr-dqn-PongNoFrameskip-v4\", \"qr-dqn-PongNoFrameskip-v4.zip\"),\n",
    "    ]\n",
    "    \n",
    "    for repo_id, filename in qrdqn_repos:\n",
    "        if qrdqn_success:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            print(f\"  Trying {repo_id}...\")\n",
    "            qrdqn_path = load_from_hub(repo_id=repo_id, filename=filename)\n",
    "            print(f\"  QRDQN file downloaded from {repo_id}\")\n",
    "            \n",
    "            # Try loading with same strategies as DQN\n",
    "            # Strategy 1: Override replay buffer settings\n",
    "            try:\n",
    "                print(\"    Strategy 1: Override replay buffer settings...\")\n",
    "                # Import QRDQN from sb3-contrib\n",
    "                try:\n",
    "                    from sb3_contrib import QRDQN\n",
    "                except ImportError:\n",
    "                    print(\"    sb3-contrib not installed, trying to import from main sb3...\")\n",
    "                    # Some versions might have QRDQN in main sb3\n",
    "                    try:\n",
    "                        from stable_baselines3 import QRDQN\n",
    "                    except ImportError:\n",
    "                        print(\"    QRDQN not available in stable-baselines3\")\n",
    "                        continue\n",
    "                \n",
    "                qrdqn_custom_objects = {\n",
    "                    \"observation_space\": env.observation_space,\n",
    "                    \"action_space\": env.action_space,\n",
    "                    \"optimize_memory_usage\": False,\n",
    "                    \"handle_timeout_termination\": False,\n",
    "                }\n",
    "                qrdqn_model = QRDQN.load(qrdqn_path, env=env, custom_objects=qrdqn_custom_objects)\n",
    "                models['qrdqn'] = qrdqn_model\n",
    "                print(\"QRDQN model downloaded successfully (strategy 1)\")\n",
    "                qrdqn_success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Strategy 1 failed: {e}\")\n",
    "                \n",
    "                # Strategy 2: Load without environment\n",
    "                try:\n",
    "                    print(\"    Strategy 2: Load without environment...\")\n",
    "                    qrdqn_model = QRDQN.load(qrdqn_path, env=None, custom_objects=qrdqn_custom_objects)\n",
    "                    qrdqn_model.set_env(env)\n",
    "                    models['qrdqn'] = qrdqn_model\n",
    "                    print(\"QRDQN model downloaded successfully (strategy 2)\")\n",
    "                    qrdqn_success = True\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    print(f\"Strategy 2 failed: {e2}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download from {repo_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not qrdqn_success:\n",
    "        print(\"All QRDQN strategies failed.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3574797-7e3b-4078-a64f-42def99054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsa_matrix(activations):\n",
    "    \"\"\"\n",
    "    Compute RSA matrix from neural activations.\n",
    "    \"\"\"\n",
    "    # Flatten activations if they're multi-dimensional\n",
    "    if len(activations.shape) > 2:\n",
    "        activations_flat = activations.reshape(activations.shape[0], -1)\n",
    "    else:\n",
    "        activations_flat = activations\n",
    "        \n",
    "    # Compute pairwise correlations\n",
    "    rsa_matrix = np.corrcoef(activations_flat)\n",
    "    \n",
    "    return rsa_matrix\n",
    "\n",
    "def compare_matrices(behavioral_matrix, neural_matrix, method='pearson'):\n",
    "    \"\"\"\n",
    "    Compare behavioral similarity matrix with neural RSA matrix.\n",
    "    \"\"\"\n",
    "    # Get upper triangular indices (excluding diagonal)\n",
    "    n = behavioral_matrix.shape[0]\n",
    "    triu_indices = np.triu_indices(n, k=1)\n",
    "    \n",
    "    behavioral_flat = behavioral_matrix[triu_indices]\n",
    "    neural_flat = neural_matrix[triu_indices]\n",
    "    \n",
    "    if method == 'pearson':\n",
    "        corr, p_val = pearsonr(behavioral_flat, neural_flat)\n",
    "    elif method == 'spearman':\n",
    "        corr, p_val = spearmanr(behavioral_flat, neural_flat)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pearson' or 'spearman'\")\n",
    "        \n",
    "    return corr, p_val\n",
    "\n",
    "def analyze_similarity_groups(behavioral_matrix, neural_matrix):\n",
    "    \"\"\"\n",
    "    Compare average neural similarity for behaviorally similar vs dissimilar states.\n",
    "    \n",
    "    Compares all corresponding entries where behavioral_matrix[i,j] == 1 vs == 0\n",
    "    with the corresponding neural_matrix[i,j] values.\n",
    "    \"\"\"\n",
    "    # Ensure matrices have the same shape\n",
    "    assert behavioral_matrix.shape == neural_matrix.shape, f\"Matrix shapes don't match: {behavioral_matrix.shape} vs {neural_matrix.shape}\"\n",
    "    \n",
    "    # Create masks for similar (1) and dissimilar (0) pairs\n",
    "    similar_mask = behavioral_matrix == 1\n",
    "    dissimilar_mask = behavioral_matrix == 0\n",
    "    \n",
    "    # Extract corresponding neural values\n",
    "    similar_neural = neural_matrix[similar_mask]\n",
    "    dissimilar_neural = neural_matrix[dissimilar_mask]\n",
    "    \n",
    "    results = {\n",
    "        'similar_mean': np.mean(similar_neural) if len(similar_neural) > 0 else np.nan,\n",
    "        'similar_std': np.std(similar_neural) if len(similar_neural) > 0 else np.nan,\n",
    "        'dissimilar_mean': np.mean(dissimilar_neural) if len(dissimilar_neural) > 0 else np.nan,\n",
    "        'dissimilar_std': np.std(dissimilar_neural) if len(dissimilar_neural) > 0 else np.nan,\n",
    "        'similar_count': len(similar_neural),\n",
    "        'dissimilar_count': len(dissimilar_neural)\n",
    "    }\n",
    "    \n",
    "    # Compute statistical test\n",
    "    if len(similar_neural) > 0 and len(dissimilar_neural) > 0:\n",
    "        from scipy.stats import ttest_ind\n",
    "        t_stat, p_val = ttest_ind(similar_neural, dissimilar_neural)\n",
    "        results['t_stat'] = t_stat\n",
    "        results['p_val'] = p_val\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4171609f-07e2-4ff8-826a-965a38fec907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models from Hugging Face...\n",
      "Downloading PPO model...\n",
      "PPO model downloaded successfully\n",
      "Attempting DQN download with multiple fallback strategies...\n",
      "  DQN file downloaded successfully\n",
      "  Strategy 1: Override replay buffer settings...\n",
      "DQN model downloaded successfully (strategy 1)\n",
      "Downloading A2C model...\n",
      "A2C model downloaded successfully\n",
      "Attempting QRDQN download...\n",
      "  Trying sb3/qrdqn-PongNoFrameskip-v4...\n",
      "  QRDQN file downloaded from sb3/qrdqn-PongNoFrameskip-v4\n",
      "    Strategy 1: Override replay buffer settings...\n",
      "    sb3-contrib not installed, trying to import from main sb3...\n",
      "    QRDQN not available in stable-baselines3\n",
      "  Trying sb3-contrib/qrdqn-PongNoFrameskip-v4...\n",
      "Failed to download from sb3-contrib/qrdqn-PongNoFrameskip-v4: 401 Client Error. (Request ID: Root=1-693b4273-1efee4d726cbe6e6004c9c0a;d3e4cad1-302b-4a8e-b7e0-0c872740ccf5)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/sb3-contrib/qrdqn-PongNoFrameskip-v4/resolve/main/qrdqn-PongNoFrameskip-v4.zip.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "  Trying sb3/qr-dqn-PongNoFrameskip-v4...\n",
      "Failed to download from sb3/qr-dqn-PongNoFrameskip-v4: 401 Client Error. (Request ID: Root=1-693b4273-605a05e362e45bf11254b441;876258d6-22b7-4870-a42d-9e79636bae79)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/sb3/qr-dqn-PongNoFrameskip-v4/resolve/main/qr-dqn-PongNoFrameskip-v4.zip.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "Invalid username or password.\n",
      "All QRDQN strategies failed.\n",
      "Downloaded 3 models: ['ppo', 'dqn', 'a2c']\n"
     ]
    }
   ],
   "source": [
    "# Download models\n",
    "print(\"Downloading models from Hugging Face...\")\n",
    "models = download_models()\n",
    "print(f\"Downloaded {len(models)} models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da5fb784-19ac-40dd-a2a0-2394f270b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate trained RL models on Pong environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name: str = \"PongNoFrameskip-v4\", render: bool = False):\n",
    "        self.env_name = env_name\n",
    "        self.render = render\n",
    "        self.evaluation_results = {}\n",
    "        \n",
    "    def create_evaluation_env(self):\n",
    "        \"\"\"Create environment for evaluation matching the training setup.\"\"\"\n",
    "        from stable_baselines3.common.env_util import make_atari_env\n",
    "        from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "        \n",
    "        # Create the same environment setup as used in training\n",
    "        if self.render:\n",
    "            env = make_atari_env(\n",
    "                self.env_name, \n",
    "                n_envs=1, \n",
    "                env_kwargs={\"render_mode\": \"human\"}\n",
    "            )\n",
    "        else:\n",
    "            env = make_atari_env(\n",
    "                self.env_name, \n",
    "                n_envs=1\n",
    "            )\n",
    "            \n",
    "        # Apply the same wrappers as in training\n",
    "        env = VecFrameStack(env, n_stack=4)\n",
    "        env = VecTransposeImage(env)\n",
    "        \n",
    "        return env\n",
    "        \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a single model over multiple episodes.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained RL model\n",
    "            model_name: Name identifier for the model\n",
    "            num_episodes: Number of evaluation episodes\n",
    "            max_episode_length: Maximum steps per episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            verbose: Print progress\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        env = self.create_evaluation_env()\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nEvaluating {model_name} model...\")\n",
    "            print(f\"Episodes: {num_episodes}, Max length: {max_episode_length}\")\n",
    "            \n",
    "        for episode in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for step in range(max_episode_length):\n",
    "                # Get action from model\n",
    "                action, _ = model.predict(obs, deterministic=deterministic)\n",
    "                \n",
    "                # Take step\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Extract scalar values from vectorized environment\n",
    "                episode_reward += reward[0]  # reward is array of length 1\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done[0]:  # done is array of length 1\n",
    "                    break\n",
    "                    \n",
    "            episode_time = time.time() - start_time\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(episode_length)\n",
    "            \n",
    "            # Determine win/loss (in Pong, positive reward means winning)\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                \n",
    "            if verbose:\n",
    "                print(f\"  Episode {episode + 1}: Reward={episode_reward:.1f}, Length={episode_length}, Time={episode_time:.2f}s\")\n",
    "                \n",
    "        env.close()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'num_episodes': num_episodes,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'mean_reward': np.mean(episode_rewards),\n",
    "            'std_reward': np.std(episode_rewards),\n",
    "            'min_reward': np.min(episode_rewards),\n",
    "            'max_reward': np.max(episode_rewards),\n",
    "            'mean_length': np.mean(episode_lengths),\n",
    "            'std_length': np.std(episode_lengths),\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'draws': num_episodes - wins - losses,\n",
    "            'win_rate': wins / num_episodes,\n",
    "            'loss_rate': losses / num_episodes\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results[model_name] = results\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{model_name.upper()} Results:\")\n",
    "            print(f\"  Mean Reward: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "            print(f\"  Win Rate: {results['win_rate']:.2%}\")\n",
    "            print(f\"  Mean Episode Length: {results['mean_length']:.1f} ± {results['std_length']:.1f}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def evaluate_all_models(\n",
    "        self,\n",
    "        models: Dict,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate all provided models.\n",
    "        \n",
    "        Args:\n",
    "            models: Dictionary of {model_name: model} pairs\n",
    "            num_episodes: Number of episodes per model\n",
    "            max_episode_length: Maximum steps per episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {len(models)} models on {self.env_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                results = self.evaluate_model(\n",
    "                    model=model,\n",
    "                    model_name=model_name,\n",
    "                    num_episodes=num_episodes,\n",
    "                    max_episode_length=max_episode_length,\n",
    "                    deterministic=deterministic\n",
    "                )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        return all_results\n",
    "        \n",
    "    def compare_models(self, results: Dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Print comparison table of model performances.\n",
    "        \n",
    "        Args:\n",
    "            results: Results dictionary, uses self.evaluation_results if None\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            results = self.evaluation_results\n",
    "            \n",
    "        if not results:\n",
    "            print(\"No evaluation results available. Run evaluate_all_models first.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':<10} {'Mean Reward':<12} {'Win Rate':<10} {'Mean Length':<12} {'Episodes':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Sort by mean reward\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_reward'], reverse=True)\n",
    "        \n",
    "        for model_name, result in sorted_results:\n",
    "            print(f\"{model_name:<10} {result['mean_reward']:>8.2f} ± {result['std_reward']:>4.2f} \"\n",
    "                  f\"{result['win_rate']:>8.1%}   {result['mean_length']:>8.1f} ± {result['std_length']:>4.1f} \"\n",
    "                  f\"{result['num_episodes']:>8d}\")\n",
    "                  \n",
    "    def plot_performance(self, results: Dict = None, save_path: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Plot model performance comparison.\n",
    "        \n",
    "        Args:\n",
    "            results: Results dictionary, uses self.evaluation_results if None\n",
    "            save_path: Path to save the plot\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            results = self.evaluation_results\n",
    "            \n",
    "        if not results:\n",
    "            print(\"No evaluation results available.\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        model_names = list(results.keys())\n",
    "        mean_rewards = [results[name]['mean_reward'] for name in model_names]\n",
    "        std_rewards = [results[name]['std_reward'] for name in model_names]\n",
    "        win_rates = [results[name]['win_rate'] for name in model_names]\n",
    "        mean_lengths = [results[name]['mean_length'] for name in model_names]\n",
    "        \n",
    "        # Mean rewards\n",
    "        axes[0, 0].bar(model_names, mean_rewards, yerr=std_rewards, capsize=5, alpha=0.7)\n",
    "        axes[0, 0].set_title('Mean Episode Reward')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Win rates\n",
    "        axes[0, 1].bar(model_names, win_rates, alpha=0.7, color='green')\n",
    "        axes[0, 1].set_title('Win Rate')\n",
    "        axes[0, 1].set_ylabel('Win Rate')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Episode lengths\n",
    "        axes[1, 0].bar(model_names, mean_lengths, alpha=0.7, color='orange')\n",
    "        axes[1, 0].set_title('Mean Episode Length')\n",
    "        axes[1, 0].set_ylabel('Steps')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reward distributions (box plot)\n",
    "        reward_data = [results[name]['episode_rewards'] for name in model_names]\n",
    "        axes[1, 1].boxplot(reward_data, labels=model_names)\n",
    "        axes[1, 1].set_title('Reward Distribution')\n",
    "        axes[1, 1].set_ylabel('Episode Reward')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "    def single_episode_demo(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        render: bool = True,\n",
    "        deterministic: bool = True,\n",
    "        max_steps: int = 5000\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Run a single episode demonstration with optional rendering.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained RL model\n",
    "            model_name: Name of the model\n",
    "            render: Whether to render the episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            max_steps: Maximum steps in the episode\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with episode information\n",
    "        \"\"\"\n",
    "        # Create environment with the same setup as evaluation\n",
    "        env = self.create_evaluation_env()\n",
    "        \n",
    "        print(f\"Running demo for {model_name} (render={render})\")\n",
    "        \n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        actions_taken = []\n",
    "        rewards_per_step = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "            actions_taken.append(action[0] if isinstance(action, np.ndarray) else action)\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward[0]\n",
    "            rewards_per_step.append(reward[0])\n",
    "            step_count += 1\n",
    "            \n",
    "            if render:\n",
    "                time.sleep(0.01)  # Slow down for viewing\n",
    "                \n",
    "            if done[0]:\n",
    "                break\n",
    "                \n",
    "        episode_time = time.time() - start_time\n",
    "        env.close()\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'total_reward': episode_reward,\n",
    "            'episode_length': step_count,\n",
    "            'episode_time': episode_time,\n",
    "            'actions_taken': actions_taken,\n",
    "            'rewards_per_step': rewards_per_step\n",
    "        }\n",
    "        \n",
    "        print(f\"Demo completed:\")\n",
    "        print(f\"  Total Reward: {episode_reward}\")\n",
    "        print(f\"  Episode Length: {step_count} steps\")\n",
    "        print(f\"  Duration: {episode_time:.2f} seconds\")\n",
    "        print(f\"  Result: {'Won' if episode_reward > 0 else 'Lost' if episode_reward < 0 else 'Draw'}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4cfbec9-91a8-4a30-b699-7d904b99d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluatorWithRSA(ModelEvaluator):\n",
    "    \"\"\"\n",
    "    Extended ModelEvaluator that also collects activations during evaluation rollouts\n",
    "    for RSA analysis using the exact same states and preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name: str = \"PongNoFrameskip-v4\", render: bool = False):\n",
    "        super().__init__(env_name, render)\n",
    "        self.rsa_results = {}\n",
    "        \n",
    "    def evaluate_model_with_rsa(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: bool = True,\n",
    "        collect_rsa: bool = True,\n",
    "        max_states_for_rsa: int = 500\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model and simultaneously collect activations for RSA analysis.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained RL model\n",
    "            model_name: Name identifier for the model\n",
    "            num_episodes: Number of evaluation episodes\n",
    "            max_episode_length: Maximum steps per episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            verbose: Print progress\n",
    "            collect_rsa: Whether to collect activations for RSA\n",
    "            max_states_for_rsa: Maximum states to collect for RSA (for computational efficiency)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics and RSA results\n",
    "        \"\"\"\n",
    "        env = self.create_evaluation_env()\n",
    "        \n",
    "        # Standard evaluation metrics\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        \n",
    "        # RSA collection\n",
    "        pixel_states = []  # States in exact model format\n",
    "        logical_states = []  # Corresponding logical states for behavioral similarity\n",
    "        all_activations = {}  # Store activations by layer\n",
    "        \n",
    "        # Set up activation extractor if needed\n",
    "        extractor = None\n",
    "        if collect_rsa:\n",
    "            extractor = ModelActivationExtractor(model, model_name)\n",
    "            extractor.register_hooks()\n",
    "            if verbose:\n",
    "                print(f\"Registered hooks for {len(extractor.hooks)} layers\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nEvaluating {model_name} model with RSA collection...\")\n",
    "            print(f\"Episodes: {num_episodes}, Max length: {max_episode_length}\")\n",
    "            print(f\"Max states for RSA: {max_states_for_rsa}\")\n",
    "            \n",
    "        for episode in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            prev_logical = None\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for step in range(max_episode_length):\n",
    "                # Check if we should collect this state for RSA\n",
    "                should_collect_rsa = (collect_rsa and len(pixel_states) < max_states_for_rsa)\n",
    "                \n",
    "                if should_collect_rsa:\n",
    "                    # Store the exact state that will be fed to the model\n",
    "                    # obs[0] is shape (4, 84, 84) - exactly what the model sees\n",
    "                    pixel_states.append(obs[0].copy())\n",
    "                    \n",
    "                    # Get corresponding logical state from RAM\n",
    "                    try:\n",
    "                        # Extract RAM from the underlying environment\n",
    "                        underlying_env = env.envs[0].unwrapped\n",
    "                        ram = underlying_env.ale.getRAM()\n",
    "                        logical_state = ram_to_logic_state(ram, prev_state=prev_logical)\n",
    "                        logical_states.append(logical_state)\n",
    "                        prev_logical = logical_state\n",
    "                    except Exception as e:\n",
    "                        if verbose and len(pixel_states) == 1:  # Only warn once\n",
    "                            print(f\"Warning: Could not extract RAM: {e}\")\n",
    "                        # Remove the pixel state we just added if RAM extraction fails\n",
    "                        pixel_states.pop()\n",
    "                        should_collect_rsa = False  # Don't collect activations either\n",
    "                \n",
    "                # Clear activations before predict call to ensure we only get this step's activations\n",
    "                if collect_rsa and extractor is not None:\n",
    "                    extractor.activations.clear()\n",
    "                \n",
    "                # Get action from model (this triggers activation collection)\n",
    "                action, _ = model.predict(obs, deterministic=deterministic)\n",
    "                \n",
    "                # Collect activations ONLY if we're collecting this state for RSA\n",
    "                if should_collect_rsa and extractor is not None and extractor.activations:\n",
    "                    # Store activations from the forward pass we just made\n",
    "                    for layer_name, activation in extractor.activations.items():\n",
    "                        if layer_name not in all_activations:\n",
    "                            all_activations[layer_name] = []\n",
    "                        # The activation should be for a single sample (batch size 1)\n",
    "                        # Take the first (and only) sample from the batch\n",
    "                        if len(activation.shape) > 1 and activation.shape[0] == 1:\n",
    "                            all_activations[layer_name].append(activation[0])  # Remove batch dimension\n",
    "                        else:\n",
    "                            all_activations[layer_name].append(activation)\n",
    "                \n",
    "                # Take step in environment\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Update episode metrics\n",
    "                episode_reward += reward[0]\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done[0]:\n",
    "                    break\n",
    "                    \n",
    "            episode_time = time.time() - start_time\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(episode_length)\n",
    "            \n",
    "            # Determine win/loss\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                \n",
    "            if verbose:\n",
    "                print(f\"  Episode {episode + 1}: Reward={episode_reward:.1f}, Length={episode_length}, \"\n",
    "                      f\"RSA States={len(pixel_states)}, Time={episode_time:.2f}s\")\n",
    "                \n",
    "        env.close()\n",
    "        \n",
    "        # Standard evaluation results\n",
    "        evaluation_results = {\n",
    "            'model_name': model_name,\n",
    "            'num_episodes': num_episodes,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'mean_reward': np.mean(episode_rewards),\n",
    "            'std_reward': np.std(episode_rewards),\n",
    "            'min_reward': np.min(episode_rewards),\n",
    "            'max_reward': np.max(episode_rewards),\n",
    "            'mean_length': np.mean(episode_lengths),\n",
    "            'std_length': np.std(episode_lengths),\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'draws': num_episodes - wins - losses,\n",
    "            'win_rate': wins / num_episodes,\n",
    "            'loss_rate': losses / num_episodes\n",
    "        }\n",
    "        \n",
    "        # RSA analysis\n",
    "        rsa_results = {}\n",
    "        if collect_rsa and len(logical_states) > 1:\n",
    "            if verbose:\n",
    "                print(f\"\\nPerforming RSA analysis with {len(logical_states)} states...\")\n",
    "                \n",
    "            # Generate behavioral similarity matrix\n",
    "            analyzer = PongSymmetryAnalyzer()\n",
    "            behavioral_matrix = analyzer.generate_similarity_matrix(logical_states)\n",
    "            analyzer.close()\n",
    "            \n",
    "            if verbose:\n",
    "                stats = analyzer.get_similarity_stats(behavioral_matrix)\n",
    "                print(\"Behavioral similarity stats:\")\n",
    "                for key, value in stats.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            \n",
    "            # Analyze each layer\n",
    "            for layer_name, layer_activations in all_activations.items():\n",
    "                if len(layer_activations) != len(logical_states):\n",
    "                    if verbose:\n",
    "                        print(f\"Warning: Mismatch in {layer_name}: {len(layer_activations)} activations vs {len(logical_states)} states\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Convert to numpy array\n",
    "                    activations_array = np.array(layer_activations)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"  Processing {layer_name} (shape: {activations_array.shape})\")\n",
    "                    \n",
    "                    # Compute RSA matrix\n",
    "                    rsa_matrix = compute_rsa_matrix(activations_array)\n",
    "                    \n",
    "                    # Compare with behavioral matrix\n",
    "                    corr_pearson, p_pearson = compare_matrices(behavioral_matrix, rsa_matrix, 'pearson')\n",
    "                    corr_spearman, p_spearman = compare_matrices(behavioral_matrix, rsa_matrix, 'spearman')\n",
    "                    \n",
    "                    # Analyze similarity groups\n",
    "                    group_analysis = analyze_similarity_groups(behavioral_matrix, rsa_matrix)\n",
    "                    \n",
    "                    rsa_results[layer_name] = {\n",
    "                        'rsa_matrix': rsa_matrix,\n",
    "                        'pearson_corr': corr_pearson,\n",
    "                        'pearson_p': p_pearson,\n",
    "                        'spearman_corr': corr_spearman,\n",
    "                        'spearman_p': p_spearman,\n",
    "                        'group_analysis': group_analysis,\n",
    "                        'n_states': len(logical_states),\n",
    "                        'activation_shape': activations_array.shape\n",
    "                    }\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"    Pearson correlation: {corr_pearson:.4f} (p={p_pearson:.4f})\")\n",
    "                        print(f\"    Spearman correlation: {corr_spearman:.4f} (p={p_spearman:.4f})\")\n",
    "                        print(f\"    Similar group mean: {group_analysis['similar_mean']:.4f} ± {group_analysis['similar_std']:.4f}\")\n",
    "                        print(f\"    Dissimilar group mean: {group_analysis['dissimilar_mean']:.4f} ± {group_analysis['dissimilar_std']:.4f}\")\n",
    "                        if 'p_val' in group_analysis:\n",
    "                            print(f\"    Group difference p-value: {group_analysis['p_val']:.4f}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Error processing {layer_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Store behavioral matrix for reference\n",
    "            rsa_results['behavioral_matrix'] = behavioral_matrix\n",
    "            rsa_results['logical_states'] = logical_states\n",
    "            rsa_results['pixel_states'] = pixel_states\n",
    "        \n",
    "        # Cleanup\n",
    "        if extractor is not None:\n",
    "            extractor.cleanup()\n",
    "        \n",
    "        # Store results\n",
    "        self.evaluation_results[model_name] = evaluation_results\n",
    "        if rsa_results:\n",
    "            self.rsa_results[model_name] = rsa_results\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\n{model_name.upper()} Results:\")\n",
    "            print(f\"  Mean Reward: {evaluation_results['mean_reward']:.2f} ± {evaluation_results['std_reward']:.2f}\")\n",
    "            print(f\"  Win Rate: {evaluation_results['win_rate']:.2%}\")\n",
    "            print(f\"  Mean Episode Length: {evaluation_results['mean_length']:.1f} ± {evaluation_results['std_length']:.1f}\")\n",
    "            if rsa_results:\n",
    "                print(f\"  RSA Analysis: {len(rsa_results)-3} layers analyzed with {len(logical_states)} states\")\n",
    "            \n",
    "        return {\n",
    "            'evaluation': evaluation_results,\n",
    "            'rsa': rsa_results\n",
    "        }\n",
    "    \n",
    "    def evaluate_all_models_with_rsa(\n",
    "        self,\n",
    "        models: Dict,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        max_states_for_rsa: int = 500\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate all models with RSA analysis.\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {len(models)} models on {self.env_name} with RSA analysis\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                results = self.evaluate_model_with_rsa(\n",
    "                    model=model,\n",
    "                    model_name=model_name,\n",
    "                    num_episodes=num_episodes,\n",
    "                    max_episode_length=max_episode_length,\n",
    "                    deterministic=deterministic,\n",
    "                    collect_rsa=True,\n",
    "                    max_states_for_rsa=max_states_for_rsa\n",
    "                )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        return all_results\n",
    "    \n",
    "    def compare_rsa_across_models(self):\n",
    "        \"\"\"\n",
    "        Compare RSA results across all evaluated models.\n",
    "        \"\"\"\n",
    "        if not self.rsa_results:\n",
    "            print(\"No RSA results available. Run evaluate_all_models_with_rsa first.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"RSA CORRELATION SUMMARY (ON-POLICY EVALUATION STATES)\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"{'Model':<10} {'Layer':<25} {'N States':<10} {'Pearson':<10} {'Spearman':<10} {'Sim Mean':<10} {'Dissim Mean':<12} {'P-value':<10}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for model_name, rsa_data in self.rsa_results.items():\n",
    "            for layer_name, layer_results in rsa_data.items():\n",
    "                if layer_name in ['behavioral_matrix', 'logical_states', 'pixel_states']:\n",
    "                    continue\n",
    "                    \n",
    "                pearson = layer_results['pearson_corr']\n",
    "                spearman = layer_results['spearman_corr']\n",
    "                n_states = layer_results['n_states']\n",
    "                group_analysis = layer_results['group_analysis']\n",
    "                \n",
    "                sim_mean = group_analysis['similar_mean']\n",
    "                dissim_mean = group_analysis['dissimilar_mean']\n",
    "                p_val = group_analysis.get('p_val', np.nan)\n",
    "                \n",
    "                print(f\"{model_name:<10} {layer_name:<25} {n_states:<10} {pearson:<10.4f} {spearman:<10.4f} {sim_mean:<10.4f} {dissim_mean:<12.4f} {p_val:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0ef57e0a-f6ab-4ec3-8d4d-df714032394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation with RSA analysis...\n",
      "Available models: ['ppo', 'dqn', 'a2c']\n",
      "Evaluating 3 models on PongNoFrameskip-v4 with RSA analysis\n",
      "======================================================================\n",
      "Network type: policy, Architecture: <class 'stable_baselines3.common.policies.ActorCriticCnnPolicy'>\n",
      "Found CNN layers in policy_features with 7 layers\n",
      "  Registered hook for policy_features_cnn_0_Conv2d\n",
      "  Registered hook for policy_features_cnn_1_ReLU\n",
      "  Registered hook for policy_features_cnn_2_Conv2d\n",
      "  Registered hook for policy_features_cnn_3_ReLU\n",
      "  Registered hook for policy_features_cnn_4_Conv2d\n",
      "  Registered hook for policy_features_cnn_5_ReLU\n",
      "  Registered hook for policy_features_cnn_6_Flatten\n",
      "Found linear layers in policy_features with 2 layers\n",
      "  Registered hook for policy_features_linear_0_Linear\n",
      "  Registered hook for policy_features_linear_1_ReLU\n",
      "Found CNN layers in value_features with 7 layers\n",
      "  Registered hook for value_features_cnn_0_Conv2d\n",
      "  Registered hook for value_features_cnn_1_ReLU\n",
      "  Registered hook for value_features_cnn_2_Conv2d\n",
      "  Registered hook for value_features_cnn_3_ReLU\n",
      "  Registered hook for value_features_cnn_4_Conv2d\n",
      "  Registered hook for value_features_cnn_5_ReLU\n",
      "  Registered hook for value_features_cnn_6_Flatten\n",
      "Found linear layers in value_features with 2 layers\n",
      "  Registered hook for value_features_linear_0_Linear\n",
      "  Registered hook for value_features_linear_1_ReLU\n",
      "Found CNN layers in shared_features with 7 layers\n",
      "  Registered hook for shared_features_cnn_0_Conv2d\n",
      "  Registered hook for shared_features_cnn_1_ReLU\n",
      "  Registered hook for shared_features_cnn_2_Conv2d\n",
      "  Registered hook for shared_features_cnn_3_ReLU\n",
      "  Registered hook for shared_features_cnn_4_Conv2d\n",
      "  Registered hook for shared_features_cnn_5_ReLU\n",
      "  Registered hook for shared_features_cnn_6_Flatten\n",
      "Found linear layers in shared_features with 2 layers\n",
      "  Registered hook for shared_features_linear_0_Linear\n",
      "  Registered hook for shared_features_linear_1_ReLU\n",
      "  Registered hook for action_net_Linear\n",
      "  Registered hook for value_net_Linear\n",
      "Total hooks registered: 29\n",
      "Feature extractors found: ['policy_features', 'value_features', 'shared_features']\n",
      "Registered hooks for 29 layers\n",
      "\n",
      "Evaluating ppo model with RSA collection...\n",
      "Episodes: 5, Max length: 10000\n",
      "Max states for RSA: 500\n",
      "  Episode 1: Reward=21.0, Length=1652, RSA States=500, Time=2.13s\n",
      "  Episode 2: Reward=21.0, Length=1664, RSA States=500, Time=2.09s\n",
      "  Episode 3: Reward=21.0, Length=1644, RSA States=500, Time=1.95s\n",
      "  Episode 4: Reward=21.0, Length=1664, RSA States=500, Time=1.99s\n",
      "  Episode 5: Reward=21.0, Length=1647, RSA States=500, Time=2.07s\n",
      "\n",
      "Performing RSA analysis with 500 states...\n",
      "Behavioral similarity stats:\n",
      "  total_states: 500\n",
      "  total_pairs: 124750\n",
      "  symmetric_pairs: 316\n",
      "  symmetry_ratio: 0.0025330661322645292\n",
      "  diagonal_sum: 500\n",
      "  Processing policy_features_cnn_0_Conv2d (shape: (500, 32, 20, 20))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  len(activations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  activations[0].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 20, 20)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  activations.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 20, 20)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m8\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m----> 8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m     10\u001b[39m         activations_flat = activations\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m13\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     11\u001b[39m \n",
      "\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Compute pairwise correlations\u001b[39;00m\n",
      "\u001b[32m---> 13\u001b[39m     rsa_matrix = np.corrcoef(activations_flat)\n",
      "\u001b[32m     14\u001b[39m \n",
      "\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rsa_matrix\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  activations_flat.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 12800)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m15\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     13\u001b[39m     rsa_matrix = np.corrcoef(activations_flat)\n",
      "\u001b[32m     14\u001b[39m \n",
      "\u001b[32m---> 15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rsa_matrix\n",
      "\u001b[32m     16\u001b[39m \n",
      "\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m compare_matrices(behavioral_matrix, neural_matrix, method=\u001b[33m'pearson'\u001b[39m):\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  rsa_matrix.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing policy_features_cnn_0_Conv2d: \n",
      "  Processing value_features_cnn_0_Conv2d (shape: (500, 32, 20, 20))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing value_features_cnn_0_Conv2d: \n",
      "  Processing shared_features_cnn_0_Conv2d (shape: (500, 32, 20, 20))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'exi' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing shared_features_cnn_0_Conv2d: \n",
      "  Processing policy_features_cnn_1_ReLU (shape: (500, 32, 20, 20))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing policy_features_cnn_1_ReLU: \n",
      "  Processing value_features_cnn_1_ReLU (shape: (500, 32, 20, 20))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing value_features_cnn_1_ReLU: \n",
      "  Processing shared_features_cnn_1_ReLU (shape: (500, 32, 20, 20))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.0240 (p=0.0000)\n",
      "    Spearman correlation: 0.0792 (p=0.0000)\n",
      "    Similar group mean: 0.9865 ± 0.0428\n",
      "    Dissimilar group mean: 0.9320 ± 0.0919\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_cnn_2_Conv2d (shape: (500, 64, 9, 9))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.0426 (p=0.0000)\n",
      "    Spearman correlation: 0.0804 (p=0.0000)\n",
      "    Similar group mean: 0.9848 ± 0.0371\n",
      "    Dissimilar group mean: 0.9193 ± 0.0631\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_cnn_2_Conv2d (shape: (500, 64, 9, 9))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.0426 (p=0.0000)\n",
      "    Spearman correlation: 0.0804 (p=0.0000)\n",
      "    Similar group mean: 0.9848 ± 0.0371\n",
      "    Dissimilar group mean: 0.9193 ± 0.0631\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_cnn_2_Conv2d (shape: (500, 64, 9, 9))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.0426 (p=0.0000)\n",
      "    Spearman correlation: 0.0804 (p=0.0000)\n",
      "    Similar group mean: 0.9848 ± 0.0371\n",
      "    Dissimilar group mean: 0.9193 ± 0.0631\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_cnn_3_ReLU (shape: (500, 64, 9, 9))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1350 (p=0.0000)\n",
      "    Spearman correlation: 0.0814 (p=0.0000)\n",
      "    Similar group mean: 0.9324 ± 0.0955\n",
      "    Dissimilar group mean: 0.6455 ± 0.0861\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_cnn_3_ReLU (shape: (500, 64, 9, 9))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1350 (p=0.0000)\n",
      "    Spearman correlation: 0.0814 (p=0.0000)\n",
      "    Similar group mean: 0.9324 ± 0.0955\n",
      "    Dissimilar group mean: 0.6455 ± 0.0861\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_cnn_3_ReLU (shape: (500, 64, 9, 9))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1350 (p=0.0000)\n",
      "    Spearman correlation: 0.0814 (p=0.0000)\n",
      "    Similar group mean: 0.9324 ± 0.0955\n",
      "    Dissimilar group mean: 0.6455 ± 0.0861\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_cnn_4_Conv2d (shape: (500, 64, 7, 7))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1610 (p=0.0000)\n",
      "    Spearman correlation: 0.0818 (p=0.0000)\n",
      "    Similar group mean: 0.9179 ± 0.1240\n",
      "    Dissimilar group mean: 0.5087 ± 0.1060\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_cnn_4_Conv2d (shape: (500, 64, 7, 7))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1610 (p=0.0000)\n",
      "    Spearman correlation: 0.0818 (p=0.0000)\n",
      "    Similar group mean: 0.9179 ± 0.1240\n",
      "    Dissimilar group mean: 0.5087 ± 0.1060\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_cnn_4_Conv2d (shape: (500, 64, 7, 7))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1610 (p=0.0000)\n",
      "    Spearman correlation: 0.0818 (p=0.0000)\n",
      "    Similar group mean: 0.9179 ± 0.1240\n",
      "    Dissimilar group mean: 0.5087 ± 0.1060\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_cnn_5_ReLU (shape: (500, 64, 7, 7))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1715 (p=0.0000)\n",
      "    Spearman correlation: 0.0819 (p=0.0000)\n",
      "    Similar group mean: 0.8788 ± 0.1781\n",
      "    Dissimilar group mean: 0.3100 ± 0.1364\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_cnn_5_ReLU (shape: (500, 64, 7, 7))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1715 (p=0.0000)\n",
      "    Spearman correlation: 0.0819 (p=0.0000)\n",
      "    Similar group mean: 0.8788 ± 0.1781\n",
      "    Dissimilar group mean: 0.3100 ± 0.1364\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_cnn_5_ReLU (shape: (500, 64, 7, 7))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1715 (p=0.0000)\n",
      "    Spearman correlation: 0.0819 (p=0.0000)\n",
      "    Similar group mean: 0.8788 ± 0.1781\n",
      "    Dissimilar group mean: 0.3100 ± 0.1364\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_cnn_6_Flatten (shape: (500, 3136))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1715 (p=0.0000)\n",
      "    Spearman correlation: 0.0819 (p=0.0000)\n",
      "    Similar group mean: 0.8788 ± 0.1781\n",
      "    Dissimilar group mean: 0.3100 ± 0.1364\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_cnn_6_Flatten (shape: (500, 3136))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1715 (p=0.0000)\n",
      "    Spearman correlation: 0.0819 (p=0.0000)\n",
      "    Similar group mean: 0.8788 ± 0.1781\n",
      "    Dissimilar group mean: 0.3100 ± 0.1364\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_cnn_6_Flatten (shape: (500, 3136))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1715 (p=0.0000)\n",
      "    Spearman correlation: 0.0819 (p=0.0000)\n",
      "    Similar group mean: 0.8788 ± 0.1781\n",
      "    Dissimilar group mean: 0.3100 ± 0.1364\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_linear_0_Linear (shape: (500, 512))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1234 (p=0.0000)\n",
      "    Spearman correlation: 0.0812 (p=0.0000)\n",
      "    Similar group mean: 0.9288 ± 0.1195\n",
      "    Dissimilar group mean: 0.4347 ± 0.1770\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_linear_0_Linear (shape: (500, 512))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1234 (p=0.0000)\n",
      "    Spearman correlation: 0.0812 (p=0.0000)\n",
      "    Similar group mean: 0.9288 ± 0.1195\n",
      "    Dissimilar group mean: 0.4347 ± 0.1770\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_linear_0_Linear (shape: (500, 512))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1234 (p=0.0000)\n",
      "    Spearman correlation: 0.0812 (p=0.0000)\n",
      "    Similar group mean: 0.9288 ± 0.1195\n",
      "    Dissimilar group mean: 0.4347 ± 0.1770\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing policy_features_linear_1_ReLU (shape: (500, 512))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1492 (p=0.0000)\n",
      "    Spearman correlation: 0.0813 (p=0.0000)\n",
      "    Similar group mean: 0.8980 ± 0.1660\n",
      "    Dissimilar group mean: 0.3011 ± 0.1720\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing value_features_linear_1_ReLU (shape: (500, 512))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1492 (p=0.0000)\n",
      "    Spearman correlation: 0.0813 (p=0.0000)\n",
      "    Similar group mean: 0.8980 ± 0.1660\n",
      "    Dissimilar group mean: 0.3011 ± 0.1720\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing shared_features_linear_1_ReLU (shape: (500, 512))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.1492 (p=0.0000)\n",
      "    Spearman correlation: 0.0813 (p=0.0000)\n",
      "    Similar group mean: 0.8980 ± 0.1660\n",
      "    Dissimilar group mean: 0.3011 ± 0.1720\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing action_net_Linear (shape: (500, 6))\n",
      "> \u001b[32m/var/folders/w5/wtxn2_3x6jgbtxczqlsckq2r0000gp/T/ipykernel_97786/1641108451.py\u001b[39m(\u001b[92m7\u001b[39m)\u001b[36mcompute_rsa_matrix\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Flatten activations if they're multi-dimensional\u001b[39;00m\n",
      "\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m----> 7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m len(activations.shape) > \u001b[32m2\u001b[39m:\n",
      "\u001b[32m      8\u001b[39m         activations_flat = activations.reshape(activations.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n",
      "\u001b[32m      9\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "    Pearson correlation: 0.0608 (p=0.0000)\n",
      "    Spearman correlation: 0.0584 (p=0.0000)\n",
      "    Similar group mean: 0.8133 ± 0.3444\n",
      "    Dissimilar group mean: 0.0606 ± 0.4994\n",
      "    Group difference p-value: 0.0000\n",
      "\n",
      "PPO Results:\n",
      "  Mean Reward: 21.00 ± 0.00\n",
      "  Win Rate: 100.00%\n",
      "  Mean Episode Length: 1654.2 ± 8.4\n",
      "  RSA Analysis: 23 layers analyzed with 500 states\n",
      "Network type: policy, Architecture: <class 'stable_baselines3.dqn.policies.CnnPolicy'>\n",
      "Found CNN layers in q_features with 7 layers\n",
      "  Registered hook for q_features_cnn_0_Conv2d\n",
      "  Registered hook for q_features_cnn_1_ReLU\n",
      "  Registered hook for q_features_cnn_2_Conv2d\n",
      "  Registered hook for q_features_cnn_3_ReLU\n",
      "  Registered hook for q_features_cnn_4_Conv2d\n",
      "  Registered hook for q_features_cnn_5_ReLU\n",
      "  Registered hook for q_features_cnn_6_Flatten\n",
      "Found linear layers in q_features with 2 layers\n",
      "  Registered hook for q_features_linear_0_Linear\n",
      "  Registered hook for q_features_linear_1_ReLU\n",
      "  Registered hook for q_net_0_Linear\n",
      "Total hooks registered: 10\n",
      "Feature extractors found: ['q_features']\n",
      "Registered hooks for 10 layers\n",
      "\n",
      "Evaluating dqn model with RSA collection...\n",
      "Episodes: 5, Max length: 10000\n",
      "Max states for RSA: 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable models: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(models.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Run evaluation with RSA collection\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Using fewer episodes but more states per episode for better RSA analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m comprehensive_results = \u001b[43mevaluator_rsa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_all_models_with_rsa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Fewer episodes but more states collected per episode\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_episode_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_states_for_rsa\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Collect up to 500 states for RSA\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m comprehensive_results:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSuccessfully evaluated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(comprehensive_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m models\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 272\u001b[39m, in \u001b[36mModelEvaluatorWithRSA.evaluate_all_models_with_rsa\u001b[39m\u001b[34m(self, models, num_episodes, max_episode_length, deterministic, max_states_for_rsa)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate_model_with_rsa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_episode_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_episode_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcollect_rsa\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_states_for_rsa\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_states_for_rsa\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m         all_results[model_name] = results\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mModelEvaluatorWithRSA.evaluate_model_with_rsa\u001b[39m\u001b[34m(self, model, model_name, num_episodes, max_episode_length, deterministic, verbose, collect_rsa, max_states_for_rsa)\u001b[39m\n\u001b[32m    114\u001b[39m             all_activations[layer_name].append(activation)\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# Take step in environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m obs, reward, done, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Update episode metrics\u001b[39;00m\n\u001b[32m    120\u001b[39m episode_reward += reward[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    217\u001b[39m \n\u001b[32m    218\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    220\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/stable_baselines3/common/vec_env/vec_transpose.py:97\u001b[39m, in \u001b[36mVecTransposeImage.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     observations, rewards, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/stable_baselines3/common/vec_env/vec_frame_stack.py:39\u001b[39m, in \u001b[36mVecFrameStack.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     33\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[32m     38\u001b[39m ]:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     observations, rewards, dones, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvenv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     observations, infos = \u001b[38;5;28mself\u001b[39m.stacked_obs.update(observations, dones, infos)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m observations, rewards, dones, infos\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[39m, in \u001b[36mDummyVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_envs):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         obs, \u001b[38;5;28mself\u001b[39m.buf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m.buf_infos[env_idx] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mself\u001b[39m.buf_dones[env_idx] = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/gymnasium/core.py:596\u001b[39m, in \u001b[36mRewardWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Modifies the :attr:`env` :meth:`step` reward using :meth:`self.reward`.\"\"\"\u001b[39;00m\n\u001b[32m    595\u001b[39m observation, reward, terminated, truncated, info = \u001b[38;5;28mself\u001b[39m.env.step(action)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/sac/lib/python3.12/site-packages/stable_baselines3/common/atari_wrappers.py:212\u001b[39m, in \u001b[36mClipRewardEnv.reward\u001b[39m\u001b[34m(self, reward)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: gym.Env) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(env)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreward\u001b[39m(\u001b[38;5;28mself\u001b[39m, reward: SupportsFloat) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Bin reward to {+1, 0, -1} by its sign.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m    216\u001b[39m \u001b[33;03m    :param reward:\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[33;03m    :return:\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.sign(\u001b[38;5;28mfloat\u001b[39m(reward))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create enhanced evaluator with RSA capabilities\n",
    "evaluator_rsa = ModelEvaluatorWithRSA(env_name=\"PongNoFrameskip-v4\", render=False)\n",
    "\n",
    "# Evaluate all models with simultaneous RSA collection\n",
    "if 'models' in locals() and models:\n",
    "    print(\"Starting comprehensive evaluation with RSA analysis...\")\n",
    "    print(f\"Available models: {list(models.keys())}\")\n",
    "    \n",
    "    # Run evaluation with RSA collection\n",
    "    # Using fewer episodes but more states per episode for better RSA analysis\n",
    "    comprehensive_results = evaluator_rsa.evaluate_all_models_with_rsa(\n",
    "        models=models,\n",
    "        num_episodes=5,  # Fewer episodes but more states collected per episode\n",
    "        max_episode_length=10000,\n",
    "        deterministic=True,\n",
    "        max_states_for_rsa=5000  # Collect up to 500 states for RSA\n",
    "    )\n",
    "    \n",
    "    if comprehensive_results:\n",
    "        print(f\"\\nSuccessfully evaluated {len(comprehensive_results)} models\")\n",
    "        \n",
    "        # Show standard performance comparison\n",
    "        evaluator_rsa.compare_models()\n",
    "        \n",
    "        # Show RSA comparison across models\n",
    "        evaluator_rsa.compare_rsa_across_models()\n",
    "        \n",
    "        # Plot performance (reuse existing method)\n",
    "        evaluator_rsa.plot_performance()\n",
    "    else:\n",
    "        print(\"No models were successfully evaluated.\")\n",
    "else:\n",
    "    print(\"No models available for evaluation. Please run the model download cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255dc476-debe-4d6c-b420-9d58a94f998b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sac",
   "language": "python",
   "name": "sac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
