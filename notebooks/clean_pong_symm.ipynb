{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50197a28-d446-4194-986d-2e906a02a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "sys.modules['gym'] = gym\n",
    "import ale_py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from huggingface_sb3 import load_from_hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02df18f3-3484-4c49-828e-687adc55f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Any, List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82187a38-5489-4336-ba52-2e0eaefa8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "PONG_RAM_INDEX = {\n",
    "    \"ball_x\":   49,\n",
    "    \"ball_y\":   54,\n",
    "    \"enemy_y\":  50,\n",
    "    \"player_y\": 51,\n",
    "}\n",
    "\n",
    "BALL_X_MIN = 50\n",
    "BALL_X_MAX = 208\n",
    "BALL_Y_MIN = 44\n",
    "BALL_Y_MAX = 207\n",
    "PLAYER_Y_MIN = 38\n",
    "PLAYER_Y_MAX = 203\n",
    "ENEMY_Y_MIN = 0\n",
    "ENEMY_Y_MAX = 208\n",
    "\n",
    "BALL_X_MID = (BALL_X_MAX - BALL_X_MIN) / 2\n",
    "BALL_Y_MID = (BALL_Y_MAX - BALL_Y_MIN) / 2\n",
    "\n",
    "PLAYER_Y_MID = (PLAYER_Y_MAX - PLAYER_Y_MIN) / 2\n",
    "ENEMY_Y_MID = (ENEMY_Y_MAX - ENEMY_Y_MIN) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7316ef21-f182-4350-8764-10793b8465f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PongSymmetryAnalyzer:\n",
    "    \"\"\"\n",
    "    A class for analyzing symmetries in Pong environments using RAM observations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Pong environment with RAM observations.\n",
    "        \n",
    "        Args:\n",
    "            render_mode: Rendering mode for the environment (None, \"human\", etc.)\n",
    "        \"\"\"\n",
    "        # Register ALE environments\n",
    "#         gym.register_envs(ale_py)\n",
    "        \n",
    "#         # Create environment with RAM observations\n",
    "#         self.env = gym.make(\n",
    "#             \"ALE/Pong-v5\",\n",
    "#             obs_type=\"ram\",\n",
    "#             render_mode=render_mode,\n",
    "#         )\n",
    "        \n",
    "        # RAM indices for extracting game state\n",
    "        self.PONG_RAM_INDEX = {\n",
    "            \"ball_x\": 49,\n",
    "            \"ball_y\": 54,\n",
    "            \"enemy_y\": 50,\n",
    "            \"player_y\": 51,\n",
    "        }\n",
    "        \n",
    "        # Game boundaries\n",
    "        self.BALL_X_MIN = 50\n",
    "        self.BALL_X_MAX = 208\n",
    "        self.BALL_Y_MIN = 44\n",
    "        self.BALL_Y_MAX = 207\n",
    "        self.PLAYER_Y_MIN = 38\n",
    "        self.PLAYER_Y_MAX = 203\n",
    "        self.ENEMY_Y_MIN = 0\n",
    "        self.ENEMY_Y_MAX = 208\n",
    "        \n",
    "        self.BALL_X_MID = (self.BALL_X_MAX - self.BALL_X_MIN) / 2\n",
    "        self.BALL_Y_MID = (self.BALL_Y_MAX - self.BALL_Y_MIN) / 2\n",
    "\n",
    "        self.PLAYER_Y_MID = (self.PLAYER_Y_MAX - self.PLAYER_Y_MIN) / 2\n",
    "        self.ENEMY_Y_MID = (self.ENEMY_Y_MAX - self.ENEMY_Y_MIN) / 2\n",
    "        \n",
    "        # Store sampled states\n",
    "        self.sampled_states: List[Dict[str, Any]] = []\n",
    "        \n",
    "    def ram_to_logic_state(\n",
    "        self, \n",
    "        ram: np.ndarray, \n",
    "        prev_state: Optional[Dict[str, Any]] = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert RAM observation to logical game state.\n",
    "        \n",
    "        Args:\n",
    "            ram: 128-byte RAM vector from the environment\n",
    "            prev_state: Previous logical state for velocity calculation\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing ball position, velocity, and paddle positions\n",
    "        \"\"\"\n",
    "        # Extract raw positions from RAM\n",
    "        ball_x_raw = int(ram[self.PONG_RAM_INDEX[\"ball_x\"]])\n",
    "        ball_y_raw = int(ram[self.PONG_RAM_INDEX[\"ball_y\"]])\n",
    "        enemy_y_raw = int(ram[self.PONG_RAM_INDEX[\"enemy_y\"]])\n",
    "        player_y_raw = int(ram[self.PONG_RAM_INDEX[\"player_y\"]])\n",
    "        \n",
    "        # Check if ball exists (OCAtari condition)\n",
    "        ball_exists = (ball_y_raw != 0) and (ball_x_raw > 49)\n",
    "        \n",
    "        # Set ball position (None if ball doesn't exist)\n",
    "        ball_x = ball_x_raw if ball_exists else None\n",
    "        ball_y = ball_y_raw if ball_exists else None\n",
    "        \n",
    "        # Paddle positions\n",
    "        player_y = player_y_raw\n",
    "        enemy_y = enemy_y_raw\n",
    "        \n",
    "        # Calculate velocity via finite differences\n",
    "        if (prev_state is not None and \n",
    "            prev_state.get(\"ball_x\") is not None and \n",
    "            ball_x is not None):\n",
    "            ball_dx = ball_x - prev_state[\"ball_x\"]\n",
    "            ball_dy = ball_y - prev_state[\"ball_y\"]\n",
    "        else:\n",
    "            ball_dx = 0\n",
    "            ball_dy = 0\n",
    "            \n",
    "        return {\n",
    "            \"ball_x\": ball_x,\n",
    "            \"ball_y\": ball_y,\n",
    "            \"ball_dx\": ball_dx,\n",
    "            \"ball_dy\": ball_dy,\n",
    "            \"player_y\": player_y,\n",
    "            \"enemy_y\": enemy_y,\n",
    "        }\n",
    "    \n",
    "    def sample_states(\n",
    "        self, \n",
    "        num_episodes: int = 5, \n",
    "        max_steps_per_episode: int = 1000,\n",
    "        model: Optional[Any] = None,\n",
    "        max_states: Optional[int] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Sample states from the environment using either a model or random policy.\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of episodes to run\n",
    "            max_steps_per_episode: Maximum steps per episode\n",
    "            model: Optional model with select_action method. If None, uses random policy\n",
    "            max_states: Maximum number of states to collect. If None, no limit\n",
    "            \n",
    "        Returns:\n",
    "            List of logical game states\n",
    "        \"\"\"\n",
    "        self.sampled_states = []\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            ram, info = self.env.reset()\n",
    "            prev_state = None\n",
    "            \n",
    "            for t in range(max_steps_per_episode):\n",
    "                # Convert RAM to logical state\n",
    "                state = self.ram_to_logic_state(ram, prev_state=prev_state)\n",
    "                self.sampled_states.append(state)\n",
    "                prev_state = state\n",
    "                \n",
    "                # Check if we've reached the maximum number of states\n",
    "                if max_states is not None and len(self.sampled_states) >= max_states:\n",
    "                    return self.sampled_states\n",
    "                \n",
    "                # Select action using model or random policy\n",
    "                if model is not None:\n",
    "                    # Assume model has a select_action method\n",
    "                    if hasattr(model, 'select_action'):\n",
    "                        action = model.select_action(ram)\n",
    "                    elif hasattr(model, 'predict'):\n",
    "                        action = model.predict(ram)\n",
    "                    else:\n",
    "                        # Try calling the model directly\n",
    "                        action = model(ram)\n",
    "                else:\n",
    "                    # Random policy\n",
    "                    action = self.env.action_space.sample()\n",
    "                \n",
    "                # Take step in environment\n",
    "                ram, reward, terminated, truncated, info = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "        return self.sampled_states\n",
    "    \n",
    "    def naive_symmetry(self, state_1: Dict[str, Any], state_2: Dict[str, Any]) -> bool:\n",
    "        \"\"\"\n",
    "        Check if two states are symmetric under naive symmetry assumption.\n",
    "        Ignores scores and opponent paddle position, bins coordinates.\n",
    "        \n",
    "        Args:\n",
    "            state_1: First game state\n",
    "            state_2: Second game state\n",
    "            \n",
    "        Returns:\n",
    "            True if states are considered symmetric, False otherwise\n",
    "        \"\"\"\n",
    "        # Check if ball x positions and x velocities match\n",
    "        ballx = state_1[\"ball_x\"] == state_2[\"ball_x\"]\n",
    "        balldx = state_1[\"ball_dx\"] == state_2[\"ball_dx\"]\n",
    "        \n",
    "        if not ballx or not balldx:\n",
    "            return False\n",
    "            \n",
    "        bally = state_1[\"ball_y\"] == state_2[\"ball_y\"]\n",
    "        balldy = state_1[\"ball_dy\"] == state_2[\"ball_dy\"]\n",
    "        playery = state_1[\"player_y\"] == state_2[\"player_y\"]\n",
    "        \n",
    "        # Case 1: Completely equal\n",
    "        if bally and balldy and playery:\n",
    "            return True\n",
    "        \n",
    "        # Case 2: Ball off screen, paddle positions symmetric\n",
    "        if state_1[\"ball_y\"] is None and state_2[\"ball_y\"] is None:\n",
    "            if (abs(state_1[\"player_y\"] - self.BALL_Y_MID) == \n",
    "                abs(state_2[\"player_y\"] - self.BALL_Y_MID)):\n",
    "                return True\n",
    "        \n",
    "        # Case 3: Symmetric reflection about y-axis\n",
    "        if (state_1[\"ball_y\"] is not None and state_2[\"ball_y\"] is not None):\n",
    "            ball_y_symmetric = (abs(state_1[\"ball_y\"] - self.BALL_Y_MID) == \n",
    "                              abs(state_2[\"ball_y\"] - self.BALL_Y_MID))\n",
    "            player_y_symmetric = (abs(state_1[\"player_y\"] - self.PLAYER_Y_MID) == \n",
    "                                abs(state_2[\"player_y\"] - self.PLAYER_Y_MID))\n",
    "            ball_dy_opposite = state_1[\"ball_dy\"] == -state_2[\"ball_dy\"]\n",
    "            \n",
    "            if ball_y_symmetric and player_y_symmetric and ball_dy_opposite:\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def generate_similarity_matrix(\n",
    "        self, \n",
    "        states: Optional[List[Dict[str, Any]]] = None,\n",
    "        symmetry_function: Optional[Callable] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a similarity matrix for the given states using a symmetry function.\n",
    "        \n",
    "        Args:\n",
    "            states: List of states to compare. If None, uses self.sampled_states\n",
    "            symmetry_function: Function to check symmetry between two states.\n",
    "                             If None, uses self.naive_symmetry\n",
    "                             \n",
    "        Returns:\n",
    "            Binary similarity matrix where entry (i,j) is 1 if states i and j are symmetric\n",
    "        \"\"\"\n",
    "        if states is None:\n",
    "            states = self.sampled_states\n",
    "            \n",
    "        if symmetry_function is None:\n",
    "            symmetry_function = self.naive_symmetry\n",
    "            \n",
    "        if len(states) == 0:\n",
    "            raise ValueError(\"No states provided for similarity matrix generation\")\n",
    "            \n",
    "        matrix = np.zeros((len(states), len(states)), dtype=int)\n",
    "        \n",
    "        for i, state_i in enumerate(states):\n",
    "            for j, state_j in enumerate(states):\n",
    "                matrix[i][j] = int(symmetry_function(state_i, state_j))\n",
    "                \n",
    "        return matrix\n",
    "    \n",
    "    def get_similarity_stats(self, similarity_matrix: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute statistics about the similarity matrix.\n",
    "        \n",
    "        Args:\n",
    "            similarity_matrix: Binary similarity matrix\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with statistics about symmetries\n",
    "        \"\"\"\n",
    "        n = similarity_matrix.shape[0]\n",
    "        total_pairs = n * (n - 1) // 2  # Exclude diagonal\n",
    "        \n",
    "        # Count symmetric pairs (excluding diagonal)\n",
    "        symmetric_pairs = (np.sum(similarity_matrix) - n) // 2\n",
    "        \n",
    "        return {\n",
    "            \"total_states\": n,\n",
    "            \"total_pairs\": total_pairs,\n",
    "            \"symmetric_pairs\": symmetric_pairs,\n",
    "            \"symmetry_ratio\": symmetric_pairs / total_pairs if total_pairs > 0 else 0.0,\n",
    "            \"diagonal_sum\": np.sum(np.diag(similarity_matrix)),\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        # self.env.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c9c7d3-67f9-4be2-88d9-653a5bee4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_symmetry(state_1, state_2):\n",
    "    \"\"\"\n",
    "    In this naive treatment, I ignore: scores, opponent paddle position.\n",
    "    I bin the x, y coordinates of the ball.\n",
    "    I bin the y coordinate of the player paddle.\n",
    "    Function returns True if: \n",
    "        - coarse x is equal in both states\n",
    "        - dx is equal in both states\n",
    "        - AND ball y and paddle y are equal OR equal on reflection in y=0 line\n",
    "    \"\"\"\n",
    "    ballx = state_1[\"ball_x\"] == state_2[\"ball_x\"]\n",
    "    balldx = state_1[\"ball_dx\"] == state_2[\"ball_dx\"]\n",
    "\n",
    "    if not ballx or not balldx:\n",
    "        return False\n",
    "        \n",
    "    bally = state_1[\"ball_y\"] == state_2[\"ball_y\"]\n",
    "    balldy = state_1[\"ball_dy\"] == state_2[\"ball_dy\"] \n",
    "\n",
    "    playery = state_1[\"player_y\"] == state_2[\"player_y\"] \n",
    "\n",
    "    # case 1: completely equal\n",
    "    # if bally:\n",
    "    #     if balldy:\n",
    "    #         if playery:\n",
    "    #             return True\n",
    "\n",
    "    # case 2: ball off screen. paddle flipped\n",
    "    if state_1[\"ball_y\"] is None:\n",
    "        if abs(state_1[\"player_y\"] - BALL_Y_MID) == abs(state_2[\"player_y\"] - BALL_Y_MID):\n",
    "            return True\n",
    "        \n",
    "    # case 2: flipped\n",
    "    if state_1[\"ball_y\"] is not None:\n",
    "        if abs(state_1[\"ball_y\"] - BALL_Y_MID) == abs(state_2[\"ball_y\"] - BALL_Y_MID):\n",
    "            if abs(state_1[\"player_y\"] - BALL_Y_MID) == abs(state_2[\"player_y\"] - BALL_Y_MID):\n",
    "                if state_1[\"ball_dy\"] == -state_2[\"ball_dy\"]:\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ac1ae4-383c-4416-b1e8-743b070efb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ram_to_logic_state(\n",
    "    ram: np.ndarray,\n",
    "    prev_state: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Minimal logic-level state for ALE Pong from a 128-byte RAM vector.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"ball_x\":   int or None,\n",
    "            \"ball_y\":   int or None,\n",
    "            \"ball_dx\":  int,\n",
    "            \"ball_dy\":  int,\n",
    "            \"player_y\": int or None,\n",
    "            \"enemy_y\":  int or None,\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Raw positions from RAM (as in OCAtari)\n",
    "    ball_x_raw   = int(ram[PONG_RAM_INDEX[\"ball_x\"]])\n",
    "    ball_y_raw   = int(ram[PONG_RAM_INDEX[\"ball_y\"]])\n",
    "    enemy_y_raw  = int(ram[PONG_RAM_INDEX[\"enemy_y\"]])\n",
    "    player_y_raw = int(ram[PONG_RAM_INDEX[\"player_y\"]])\n",
    "\n",
    "    # OCAtari condition for “ball exists”\n",
    "    ball_exists = (ball_y_raw != 0) and (ball_x_raw > 49)\n",
    "\n",
    "    # If you want to treat \"no ball\" explicitly:\n",
    "    ball_x = ball_x_raw if ball_exists else None\n",
    "    ball_y = ball_y_raw if ball_exists else None\n",
    "\n",
    "    # Paddles basically always exist when game is running;\n",
    "    # if you want to mirror OCAtari, you could add checks similar to ram[50] / ram[51] ranges.\n",
    "    player_y = player_y_raw\n",
    "    enemy_y  = enemy_y_raw\n",
    "\n",
    "    # Velocity via finite differences\n",
    "    if prev_state is not None and prev_state.get(\"ball_x\") is not None and ball_x is not None:\n",
    "        ball_dx = ball_x - prev_state[\"ball_x\"]\n",
    "        ball_dy = ball_y - prev_state[\"ball_y\"]\n",
    "    else:\n",
    "        ball_dx = 0\n",
    "        ball_dy = 0\n",
    "\n",
    "    return {\n",
    "        \"ball_x\":   ball_x,\n",
    "        \"ball_y\":   ball_y,\n",
    "        \"ball_dx\":  ball_dx,\n",
    "        \"ball_dy\":  ball_dy,\n",
    "        \"player_y\": player_y,\n",
    "        \"enemy_y\":  enemy_y,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e167bbc1-97fe-4523-b3ed-350c9b095dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelActivationExtractor:\n",
    "    \"\"\"\n",
    "    Extract activations from different layers of trained RL models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_type='ppo'):\n",
    "        self.model = model\n",
    "        self.model_type = model_type.lower()\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self, layer_names=None):\n",
    "        \"\"\"Register forward hooks to capture activations.\"\"\"\n",
    "        def hook_fn(name):\n",
    "            def hook(module, input, output):\n",
    "                if isinstance(output, torch.Tensor):\n",
    "                    self.activations[name] = output.detach().cpu().numpy()\n",
    "                elif isinstance(output, (list, tuple)):\n",
    "                    # Handle cases where output is a tuple/list\n",
    "                    self.activations[name] = output[0].detach().cpu().numpy()\n",
    "            return hook\n",
    "            \n",
    "        # Get the policy network\n",
    "        if hasattr(self.model, 'policy'):\n",
    "            policy_net = self.model.policy\n",
    "        else:\n",
    "            policy_net = self.model\n",
    "            \n",
    "        # Register hooks for different model architectures\n",
    "        if hasattr(policy_net, 'features_extractor'):\n",
    "            # CNN-based models\n",
    "            cnn = policy_net.features_extractor.cnn\n",
    "            for i, layer in enumerate(cnn):\n",
    "                if isinstance(layer, (nn.Conv2d, nn.Linear, nn.ReLU)):\n",
    "                    name = f'cnn_layer_{i}_{layer.__class__.__name__}'\n",
    "                    if layer_names is None or name in layer_names:\n",
    "                        hook = layer.register_forward_hook(hook_fn(name))\n",
    "                        self.hooks.append(hook)\n",
    "                        \n",
    "        # Value and policy heads - handle single layers\n",
    "        if hasattr(policy_net, 'value_net'):\n",
    "            # Check if it's a single layer or a sequence\n",
    "            if isinstance(policy_net.value_net, nn.Sequential):\n",
    "                for i, layer in enumerate(policy_net.value_net):\n",
    "                    if isinstance(layer, (nn.Linear, nn.ReLU)):\n",
    "                        name = f'value_net_{i}_{layer.__class__.__name__}'\n",
    "                        if layer_names is None or name in layer_names:\n",
    "                            hook = layer.register_forward_hook(hook_fn(name))\n",
    "                            self.hooks.append(hook)\n",
    "            else:\n",
    "                # Single layer\n",
    "                name = f'value_net_{policy_net.value_net.__class__.__name__}'\n",
    "                if layer_names is None or name in layer_names:\n",
    "                    hook = policy_net.value_net.register_forward_hook(hook_fn(name))\n",
    "                    self.hooks.append(hook)\n",
    "                    \n",
    "        if hasattr(policy_net, 'action_net'):\n",
    "            # Check if it's a single layer or a sequence\n",
    "            if isinstance(policy_net.action_net, nn.Sequential):\n",
    "                for i, layer in enumerate(policy_net.action_net):\n",
    "                    if isinstance(layer, (nn.Linear, nn.ReLU)):\n",
    "                        name = f'action_net_{i}_{layer.__class__.__name__}'\n",
    "                        if layer_names is None or name in layer_names:\n",
    "                            hook = layer.register_forward_hook(hook_fn(name))\n",
    "                            self.hooks.append(hook)\n",
    "            else:\n",
    "                # Single layer\n",
    "                name = f'action_net_{policy_net.action_net.__class__.__name__}'\n",
    "                if layer_names is None or name in layer_names:\n",
    "                    hook = policy_net.action_net.register_forward_hook(hook_fn(name))\n",
    "                    self.hooks.append(hook)\n",
    "    \n",
    "    def extract_activations(self, states):\n",
    "        \"\"\"Extract activations for a batch of states.\"\"\"\n",
    "        self.activations.clear()\n",
    "        \n",
    "        # Convert states to tensor\n",
    "        if isinstance(states, np.ndarray):\n",
    "            states_tensor = torch.FloatTensor(states)\n",
    "        else:\n",
    "            states_tensor = torch.FloatTensor(np.array(states))\n",
    "            \n",
    "        # Ensure correct shape (batch_size, channels, height, width)\n",
    "        if len(states_tensor.shape) == 3:\n",
    "            states_tensor = states_tensor.unsqueeze(0)\n",
    "        if states_tensor.shape[-1] == 3:  # If channels last\n",
    "            states_tensor = states_tensor.permute(0, 3, 1, 2)\n",
    "            \n",
    "        # Normalize to [0, 1] if needed\n",
    "        if states_tensor.max() > 1.0:\n",
    "            states_tensor = states_tensor / 255.0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            if self.model_type in ['ppo', 'a2c']:\n",
    "                # For policy gradient methods\n",
    "                self.model.policy(states_tensor)\n",
    "            elif self.model_type == 'dqn':\n",
    "                # For Q-learning methods\n",
    "                self.model.q_net(states_tensor)\n",
    "                \n",
    "        return dict(self.activations)\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove all registered hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d789c43-8fff-49ec-ae95-3a25d99dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_models(env_name: str = \"PongNoFrameskip-v4\"):\n",
    "    \"\"\"Download pre-trained models from Hugging Face.\"\"\"\n",
    "    models = {}\n",
    "\n",
    "    # pre-create environment\n",
    "    # env = gym.make(env_name, render_mode=\"human\")\n",
    "    # env = ChannelFirstWrapper(env)\n",
    "    env = make_atari_env(env_name, n_envs=1, env_kwargs={\"render_mode\": \"human\"})\n",
    "    env = VecFrameStack(env, n_stack=4)\n",
    "    env = VecTransposeImage(env)\n",
    "\n",
    "    # 4. Load with CUSTOM_OBJECTS override\n",
    "    # This replaces the metadata from the file with the spaces from your current env\n",
    "    custom_objects = {\n",
    "        \"observation_space\": env.observation_space,\n",
    "        \"action_space\": env.action_space\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # PPO model\n",
    "        print(\"Downloading PPO model...\")\n",
    "        ppo_path = load_from_hub(\n",
    "            repo_id=f\"sb3/ppo-{env_name}\",\n",
    "            filename=f\"ppo-{env_name}.zip\"\n",
    "        )\n",
    "        ppo_model = PPO.load(ppo_path, env=env, custom_objects=custom_objects)\n",
    "        models['ppo'] = ppo_model\n",
    "        print(\"PPO model downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download PPO model: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # DQN model\n",
    "        print(\"Downloading DQN model...\")\n",
    "        dqn_path = load_from_hub(\n",
    "            repo_id=f\"sb3/dqn-{env_name}\", \n",
    "            filename=f\"dqn-{env_name}.zip\"\n",
    "        )\n",
    "        dqn_model = DQN.load(dqn_path, env=env, custom_objects=custom_objects)\n",
    "        models['dqn'] = dqn_model\n",
    "        print(\"DQN model downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download DQN model: {e}\")\n",
    "        \n",
    "    try:\n",
    "        # A2C model\n",
    "        print(\"Downloading A2C model...\")\n",
    "        a2c_path = load_from_hub(\n",
    "            repo_id=f\"sb3/a2c-{env_name}\",\n",
    "            filename=f\"a2c-{env_name}.zip\"\n",
    "        )\n",
    "        a2c_model = A2C.load(a2c_path, env=env, custom_objects=custom_objects)\n",
    "        models['a2c'] = a2c_model\n",
    "        print(\"A2C model downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download A2C model: {e}\")\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3574797-7e3b-4078-a64f-42def99054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsa_matrix(activations):\n",
    "    \"\"\"\n",
    "    Compute RSA matrix from neural activations.\n",
    "    \"\"\"\n",
    "    # Flatten activations if they're multi-dimensional\n",
    "    if len(activations.shape) > 2:\n",
    "        activations_flat = activations.reshape(activations.shape[0], -1)\n",
    "    else:\n",
    "        activations_flat = activations\n",
    "        \n",
    "    # Compute pairwise correlations\n",
    "    rsa_matrix = np.corrcoef(activations_flat)\n",
    "    \n",
    "    return rsa_matrix\n",
    "\n",
    "def compare_matrices(behavioral_matrix, neural_matrix, method='pearson'):\n",
    "    \"\"\"\n",
    "    Compare behavioral similarity matrix with neural RSA matrix.\n",
    "    \"\"\"\n",
    "    # Get upper triangular indices (excluding diagonal)\n",
    "    n = behavioral_matrix.shape[0]\n",
    "    triu_indices = np.triu_indices(n, k=1)\n",
    "    \n",
    "    behavioral_flat = behavioral_matrix[triu_indices]\n",
    "    neural_flat = neural_matrix[triu_indices]\n",
    "    \n",
    "    if method == 'pearson':\n",
    "        corr, p_val = pearsonr(behavioral_flat, neural_flat)\n",
    "    elif method == 'spearman':\n",
    "        corr, p_val = spearmanr(behavioral_flat, neural_flat)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pearson' or 'spearman'\")\n",
    "        \n",
    "    return corr, p_val\n",
    "\n",
    "def analyze_similarity_groups(behavioral_matrix, neural_matrix):\n",
    "    \"\"\"\n",
    "    Compare average neural similarity for behaviorally similar vs dissimilar states.\n",
    "    \n",
    "    Compares all corresponding entries where behavioral_matrix[i,j] == 1 vs == 0\n",
    "    with the corresponding neural_matrix[i,j] values.\n",
    "    \"\"\"\n",
    "    # Ensure matrices have the same shape\n",
    "    assert behavioral_matrix.shape == neural_matrix.shape, f\"Matrix shapes don't match: {behavioral_matrix.shape} vs {neural_matrix.shape}\"\n",
    "    \n",
    "    # Create masks for similar (1) and dissimilar (0) pairs\n",
    "    similar_mask = behavioral_matrix == 1\n",
    "    dissimilar_mask = behavioral_matrix == 0\n",
    "    \n",
    "    # Extract corresponding neural values\n",
    "    similar_neural = neural_matrix[similar_mask]\n",
    "    dissimilar_neural = neural_matrix[dissimilar_mask]\n",
    "    \n",
    "    results = {\n",
    "        'similar_mean': np.mean(similar_neural) if len(similar_neural) > 0 else np.nan,\n",
    "        'similar_std': np.std(similar_neural) if len(similar_neural) > 0 else np.nan,\n",
    "        'dissimilar_mean': np.mean(dissimilar_neural) if len(dissimilar_neural) > 0 else np.nan,\n",
    "        'dissimilar_std': np.std(dissimilar_neural) if len(dissimilar_neural) > 0 else np.nan,\n",
    "        'similar_count': len(similar_neural),\n",
    "        'dissimilar_count': len(dissimilar_neural)\n",
    "    }\n",
    "    \n",
    "    # Compute statistical test\n",
    "    if len(similar_neural) > 0 and len(dissimilar_neural) > 0:\n",
    "        from scipy.stats import ttest_ind\n",
    "        t_stat, p_val = ttest_ind(similar_neural, dissimilar_neural)\n",
    "        results['t_stat'] = t_stat\n",
    "        results['p_val'] = p_val\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4171609f-07e2-4ff8-826a-965a38fec907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading models from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading PPO model...\n",
      "PPO model downloaded successfully\n",
      "Downloading DQN model...\n",
      "Failed to download DQN model: ReplayBuffer does not support optimize_memory_usage = True and handle_timeout_termination = True simultaneously.\n",
      "Downloading A2C model...\n",
      "A2C model downloaded successfully\n",
      "Downloaded 2 models: ['ppo', 'a2c']\n"
     ]
    }
   ],
   "source": [
    "# Download models\n",
    "print(\"Downloading models from Hugging Face...\")\n",
    "models = download_models()\n",
    "print(f\"Downloaded {len(models)} models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da5fb784-19ac-40dd-a2a0-2394f270b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate trained RL models on Pong environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name: str = \"PongNoFrameskip-v4\", render: bool = False):\n",
    "        self.env_name = env_name\n",
    "        self.render = render\n",
    "        self.evaluation_results = {}\n",
    "        \n",
    "    def create_evaluation_env(self):\n",
    "        \"\"\"Create environment for evaluation matching the training setup.\"\"\"\n",
    "        from stable_baselines3.common.env_util import make_atari_env\n",
    "        from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "        \n",
    "        # Create the same environment setup as used in training\n",
    "        if self.render:\n",
    "            env = make_atari_env(\n",
    "                self.env_name, \n",
    "                n_envs=1, \n",
    "                env_kwargs={\"render_mode\": \"human\"}\n",
    "            )\n",
    "        else:\n",
    "            env = make_atari_env(\n",
    "                self.env_name, \n",
    "                n_envs=1\n",
    "            )\n",
    "            \n",
    "        # Apply the same wrappers as in training\n",
    "        env = VecFrameStack(env, n_stack=4)\n",
    "        env = VecTransposeImage(env)\n",
    "        \n",
    "        return env\n",
    "        \n",
    "    def evaluate_model(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a single model over multiple episodes.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained RL model\n",
    "            model_name: Name identifier for the model\n",
    "            num_episodes: Number of evaluation episodes\n",
    "            max_episode_length: Maximum steps per episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            verbose: Print progress\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        env = self.create_evaluation_env()\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nEvaluating {model_name} model...\")\n",
    "            print(f\"Episodes: {num_episodes}, Max length: {max_episode_length}\")\n",
    "            \n",
    "        for episode in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for step in range(max_episode_length):\n",
    "                # Get action from model\n",
    "                action, _ = model.predict(obs, deterministic=deterministic)\n",
    "                \n",
    "                # Take step\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Extract scalar values from vectorized environment\n",
    "                episode_reward += reward[0]  # reward is array of length 1\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done[0]:  # done is array of length 1\n",
    "                    break\n",
    "                    \n",
    "            episode_time = time.time() - start_time\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(episode_length)\n",
    "            \n",
    "            # Determine win/loss (in Pong, positive reward means winning)\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                \n",
    "            if verbose:\n",
    "                print(f\"  Episode {episode + 1}: Reward={episode_reward:.1f}, Length={episode_length}, Time={episode_time:.2f}s\")\n",
    "                \n",
    "        env.close()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        results = {\n",
    "            'model_name': model_name,\n",
    "            'num_episodes': num_episodes,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'mean_reward': np.mean(episode_rewards),\n",
    "            'std_reward': np.std(episode_rewards),\n",
    "            'min_reward': np.min(episode_rewards),\n",
    "            'max_reward': np.max(episode_rewards),\n",
    "            'mean_length': np.mean(episode_lengths),\n",
    "            'std_length': np.std(episode_lengths),\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'draws': num_episodes - wins - losses,\n",
    "            'win_rate': wins / num_episodes,\n",
    "            'loss_rate': losses / num_episodes\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results[model_name] = results\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{model_name.upper()} Results:\")\n",
    "            print(f\"  Mean Reward: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "            print(f\"  Win Rate: {results['win_rate']:.2%}\")\n",
    "            print(f\"  Mean Episode Length: {results['mean_length']:.1f} ± {results['std_length']:.1f}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def evaluate_all_models(\n",
    "        self,\n",
    "        models: Dict,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate all provided models.\n",
    "        \n",
    "        Args:\n",
    "            models: Dictionary of {model_name: model} pairs\n",
    "            num_episodes: Number of episodes per model\n",
    "            max_episode_length: Maximum steps per episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all evaluation results\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {len(models)} models on {self.env_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                results = self.evaluate_model(\n",
    "                    model=model,\n",
    "                    model_name=model_name,\n",
    "                    num_episodes=num_episodes,\n",
    "                    max_episode_length=max_episode_length,\n",
    "                    deterministic=deterministic\n",
    "                )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        return all_results\n",
    "        \n",
    "    def compare_models(self, results: Dict = None) -> None:\n",
    "        \"\"\"\n",
    "        Print comparison table of model performances.\n",
    "        \n",
    "        Args:\n",
    "            results: Results dictionary, uses self.evaluation_results if None\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            results = self.evaluation_results\n",
    "            \n",
    "        if not results:\n",
    "            print(\"No evaluation results available. Run evaluate_all_models first.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':<10} {'Mean Reward':<12} {'Win Rate':<10} {'Mean Length':<12} {'Episodes':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Sort by mean reward\n",
    "        sorted_results = sorted(results.items(), key=lambda x: x[1]['mean_reward'], reverse=True)\n",
    "        \n",
    "        for model_name, result in sorted_results:\n",
    "            print(f\"{model_name:<10} {result['mean_reward']:>8.2f} ± {result['std_reward']:>4.2f} \"\n",
    "                  f\"{result['win_rate']:>8.1%}   {result['mean_length']:>8.1f} ± {result['std_length']:>4.1f} \"\n",
    "                  f\"{result['num_episodes']:>8d}\")\n",
    "                  \n",
    "    def plot_performance(self, results: Dict = None, save_path: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Plot model performance comparison.\n",
    "        \n",
    "        Args:\n",
    "            results: Results dictionary, uses self.evaluation_results if None\n",
    "            save_path: Path to save the plot\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            results = self.evaluation_results\n",
    "            \n",
    "        if not results:\n",
    "            print(\"No evaluation results available.\")\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        model_names = list(results.keys())\n",
    "        mean_rewards = [results[name]['mean_reward'] for name in model_names]\n",
    "        std_rewards = [results[name]['std_reward'] for name in model_names]\n",
    "        win_rates = [results[name]['win_rate'] for name in model_names]\n",
    "        mean_lengths = [results[name]['mean_length'] for name in model_names]\n",
    "        \n",
    "        # Mean rewards\n",
    "        axes[0, 0].bar(model_names, mean_rewards, yerr=std_rewards, capsize=5, alpha=0.7)\n",
    "        axes[0, 0].set_title('Mean Episode Reward')\n",
    "        axes[0, 0].set_ylabel('Reward')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Win rates\n",
    "        axes[0, 1].bar(model_names, win_rates, alpha=0.7, color='green')\n",
    "        axes[0, 1].set_title('Win Rate')\n",
    "        axes[0, 1].set_ylabel('Win Rate')\n",
    "        axes[0, 1].set_ylim(0, 1)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Episode lengths\n",
    "        axes[1, 0].bar(model_names, mean_lengths, alpha=0.7, color='orange')\n",
    "        axes[1, 0].set_title('Mean Episode Length')\n",
    "        axes[1, 0].set_ylabel('Steps')\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reward distributions (box plot)\n",
    "        reward_data = [results[name]['episode_rewards'] for name in model_names]\n",
    "        axes[1, 1].boxplot(reward_data, labels=model_names)\n",
    "        axes[1, 1].set_title('Reward Distribution')\n",
    "        axes[1, 1].set_ylabel('Episode Reward')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "    def single_episode_demo(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        render: bool = True,\n",
    "        deterministic: bool = True,\n",
    "        max_steps: int = 5000\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Run a single episode demonstration with optional rendering.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained RL model\n",
    "            model_name: Name of the model\n",
    "            render: Whether to render the episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            max_steps: Maximum steps in the episode\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with episode information\n",
    "        \"\"\"\n",
    "        # Create environment with the same setup as evaluation\n",
    "        env = self.create_evaluation_env()\n",
    "        \n",
    "        print(f\"Running demo for {model_name} (render={render})\")\n",
    "        \n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        actions_taken = []\n",
    "        rewards_per_step = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action, _ = model.predict(obs, deterministic=deterministic)\n",
    "            actions_taken.append(action[0] if isinstance(action, np.ndarray) else action)\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward[0]\n",
    "            rewards_per_step.append(reward[0])\n",
    "            step_count += 1\n",
    "            \n",
    "            if render:\n",
    "                time.sleep(0.01)  # Slow down for viewing\n",
    "                \n",
    "            if done[0]:\n",
    "                break\n",
    "                \n",
    "        episode_time = time.time() - start_time\n",
    "        env.close()\n",
    "        \n",
    "        result = {\n",
    "            'model_name': model_name,\n",
    "            'total_reward': episode_reward,\n",
    "            'episode_length': step_count,\n",
    "            'episode_time': episode_time,\n",
    "            'actions_taken': actions_taken,\n",
    "            'rewards_per_step': rewards_per_step\n",
    "        }\n",
    "        \n",
    "        print(f\"Demo completed:\")\n",
    "        print(f\"  Total Reward: {episode_reward}\")\n",
    "        print(f\"  Episode Length: {step_count} steps\")\n",
    "        print(f\"  Duration: {episode_time:.2f} seconds\")\n",
    "        print(f\"  Result: {'Won' if episode_reward > 0 else 'Lost' if episode_reward < 0 else 'Draw'}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4cfbec9-91a8-4a30-b699-7d904b99d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluatorWithRSA(ModelEvaluator):\n",
    "    \"\"\"\n",
    "    Extended ModelEvaluator that also collects activations during evaluation rollouts\n",
    "    for RSA analysis using the exact same states and preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env_name: str = \"PongNoFrameskip-v4\", render: bool = False):\n",
    "        super().__init__(env_name, render)\n",
    "        self.rsa_results = {}\n",
    "        \n",
    "    def evaluate_model_with_rsa(\n",
    "        self,\n",
    "        model,\n",
    "        model_name: str,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: bool = True,\n",
    "        collect_rsa: bool = True,\n",
    "        max_states_for_rsa: int = 500\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate model and simultaneously collect activations for RSA analysis.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained RL model\n",
    "            model_name: Name identifier for the model\n",
    "            num_episodes: Number of evaluation episodes\n",
    "            max_episode_length: Maximum steps per episode\n",
    "            deterministic: Whether to use deterministic policy\n",
    "            verbose: Print progress\n",
    "            collect_rsa: Whether to collect activations for RSA\n",
    "            max_states_for_rsa: Maximum states to collect for RSA (for computational efficiency)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics and RSA results\n",
    "        \"\"\"\n",
    "        env = self.create_evaluation_env()\n",
    "        \n",
    "        # Standard evaluation metrics\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        \n",
    "        # RSA collection\n",
    "        pixel_states = []  # States in exact model format\n",
    "        logical_states = []  # Corresponding logical states for behavioral similarity\n",
    "        all_activations = {}  # Store activations by layer\n",
    "        \n",
    "        # Set up activation extractor if needed\n",
    "        extractor = None\n",
    "        if collect_rsa:\n",
    "            extractor = ModelActivationExtractor(model, model_name)\n",
    "            extractor.register_hooks()\n",
    "            if verbose:\n",
    "                print(f\"Registered hooks for {len(extractor.hooks)} layers\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nEvaluating {model_name} model with RSA collection...\")\n",
    "            print(f\"Episodes: {num_episodes}, Max length: {max_episode_length}\")\n",
    "            print(f\"Max states for RSA: {max_states_for_rsa}\")\n",
    "            \n",
    "        for episode in range(num_episodes):\n",
    "            obs = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            prev_logical = None\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for step in range(max_episode_length):\n",
    "                # Check if we should collect this state for RSA\n",
    "                should_collect_rsa = (collect_rsa and len(pixel_states) < max_states_for_rsa)\n",
    "                \n",
    "                if should_collect_rsa:\n",
    "                    # Store the exact state that will be fed to the model\n",
    "                    # obs[0] is shape (4, 84, 84) - exactly what the model sees\n",
    "                    pixel_states.append(obs[0].copy())\n",
    "                    \n",
    "                    # Get corresponding logical state from RAM\n",
    "                    try:\n",
    "                        # Extract RAM from the underlying environment\n",
    "                        underlying_env = env.envs[0].unwrapped\n",
    "                        ram = underlying_env.ale.getRAM()\n",
    "                        logical_state = ram_to_logic_state(ram, prev_state=prev_logical)\n",
    "                        logical_states.append(logical_state)\n",
    "                        prev_logical = logical_state\n",
    "                    except Exception as e:\n",
    "                        if verbose and len(pixel_states) == 1:  # Only warn once\n",
    "                            print(f\"Warning: Could not extract RAM: {e}\")\n",
    "                        # Remove the pixel state we just added if RAM extraction fails\n",
    "                        pixel_states.pop()\n",
    "                        should_collect_rsa = False  # Don't collect activations either\n",
    "                \n",
    "                # Clear activations before predict call to ensure we only get this step's activations\n",
    "                if collect_rsa and extractor is not None:\n",
    "                    extractor.activations.clear()\n",
    "                \n",
    "                # Get action from model (this triggers activation collection)\n",
    "                action, _ = model.predict(obs, deterministic=deterministic)\n",
    "                \n",
    "                # Collect activations ONLY if we're collecting this state for RSA\n",
    "                if should_collect_rsa and extractor is not None and extractor.activations:\n",
    "                    # Store activations from the forward pass we just made\n",
    "                    for layer_name, activation in extractor.activations.items():\n",
    "                        if layer_name not in all_activations:\n",
    "                            all_activations[layer_name] = []\n",
    "                        # The activation should be for a single sample (batch size 1)\n",
    "                        # Take the first (and only) sample from the batch\n",
    "                        if len(activation.shape) > 1 and activation.shape[0] == 1:\n",
    "                            all_activations[layer_name].append(activation[0])  # Remove batch dimension\n",
    "                        else:\n",
    "                            all_activations[layer_name].append(activation)\n",
    "                \n",
    "                # Take step in environment\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Update episode metrics\n",
    "                episode_reward += reward[0]\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done[0]:\n",
    "                    break\n",
    "                    \n",
    "            episode_time = time.time() - start_time\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(episode_length)\n",
    "            \n",
    "            # Determine win/loss\n",
    "            if episode_reward > 0:\n",
    "                wins += 1\n",
    "            elif episode_reward < 0:\n",
    "                losses += 1\n",
    "                \n",
    "            if verbose:\n",
    "                print(f\"  Episode {episode + 1}: Reward={episode_reward:.1f}, Length={episode_length}, \"\n",
    "                      f\"RSA States={len(pixel_states)}, Time={episode_time:.2f}s\")\n",
    "                \n",
    "        env.close()\n",
    "        \n",
    "        # Standard evaluation results\n",
    "        evaluation_results = {\n",
    "            'model_name': model_name,\n",
    "            'num_episodes': num_episodes,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'episode_lengths': episode_lengths,\n",
    "            'mean_reward': np.mean(episode_rewards),\n",
    "            'std_reward': np.std(episode_rewards),\n",
    "            'min_reward': np.min(episode_rewards),\n",
    "            'max_reward': np.max(episode_rewards),\n",
    "            'mean_length': np.mean(episode_lengths),\n",
    "            'std_length': np.std(episode_lengths),\n",
    "            'wins': wins,\n",
    "            'losses': losses,\n",
    "            'draws': num_episodes - wins - losses,\n",
    "            'win_rate': wins / num_episodes,\n",
    "            'loss_rate': losses / num_episodes\n",
    "        }\n",
    "        \n",
    "        # RSA analysis\n",
    "        rsa_results = {}\n",
    "        if collect_rsa and len(logical_states) > 1:\n",
    "            if verbose:\n",
    "                print(f\"\\nPerforming RSA analysis with {len(logical_states)} states...\")\n",
    "                \n",
    "            # Generate behavioral similarity matrix\n",
    "            analyzer = PongSymmetryAnalyzer()\n",
    "            behavioral_matrix = analyzer.generate_similarity_matrix(logical_states)\n",
    "            analyzer.close()\n",
    "            \n",
    "            if verbose:\n",
    "                stats = analyzer.get_similarity_stats(behavioral_matrix)\n",
    "                print(\"Behavioral similarity stats:\")\n",
    "                for key, value in stats.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            \n",
    "            # Analyze each layer\n",
    "            for layer_name, layer_activations in all_activations.items():\n",
    "                if len(layer_activations) != len(logical_states):\n",
    "                    if verbose:\n",
    "                        print(f\"Warning: Mismatch in {layer_name}: {len(layer_activations)} activations vs {len(logical_states)} states\")\n",
    "                    continue\n",
    "                    \n",
    "                try:\n",
    "                    # Convert to numpy array\n",
    "                    activations_array = np.array(layer_activations)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"  Processing {layer_name} (shape: {activations_array.shape})\")\n",
    "                    \n",
    "                    # Compute RSA matrix\n",
    "                    rsa_matrix = compute_rsa_matrix(activations_array)\n",
    "                    \n",
    "                    # Compare with behavioral matrix\n",
    "                    corr_pearson, p_pearson = compare_matrices(behavioral_matrix, rsa_matrix, 'pearson')\n",
    "                    corr_spearman, p_spearman = compare_matrices(behavioral_matrix, rsa_matrix, 'spearman')\n",
    "                    \n",
    "                    # Analyze similarity groups\n",
    "                    group_analysis = analyze_similarity_groups(behavioral_matrix, rsa_matrix)\n",
    "                    \n",
    "                    rsa_results[layer_name] = {\n",
    "                        'rsa_matrix': rsa_matrix,\n",
    "                        'pearson_corr': corr_pearson,\n",
    "                        'pearson_p': p_pearson,\n",
    "                        'spearman_corr': corr_spearman,\n",
    "                        'spearman_p': p_spearman,\n",
    "                        'group_analysis': group_analysis,\n",
    "                        'n_states': len(logical_states),\n",
    "                        'activation_shape': activations_array.shape\n",
    "                    }\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(f\"    Pearson correlation: {corr_pearson:.4f} (p={p_pearson:.4f})\")\n",
    "                        print(f\"    Spearman correlation: {corr_spearman:.4f} (p={p_spearman:.4f})\")\n",
    "                        print(f\"    Similar group mean: {group_analysis['similar_mean']:.4f} ± {group_analysis['similar_std']:.4f}\")\n",
    "                        print(f\"    Dissimilar group mean: {group_analysis['dissimilar_mean']:.4f} ± {group_analysis['dissimilar_std']:.4f}\")\n",
    "                        if 'p_val' in group_analysis:\n",
    "                            print(f\"    Group difference p-value: {group_analysis['p_val']:.4f}\")\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    if verbose:\n",
    "                        print(f\"Error processing {layer_name}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Store behavioral matrix for reference\n",
    "            rsa_results['behavioral_matrix'] = behavioral_matrix\n",
    "            rsa_results['logical_states'] = logical_states\n",
    "            rsa_results['pixel_states'] = pixel_states\n",
    "        \n",
    "        # Cleanup\n",
    "        if extractor is not None:\n",
    "            extractor.cleanup()\n",
    "        \n",
    "        # Store results\n",
    "        self.evaluation_results[model_name] = evaluation_results\n",
    "        if rsa_results:\n",
    "            self.rsa_results[model_name] = rsa_results\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\n{model_name.upper()} Results:\")\n",
    "            print(f\"  Mean Reward: {evaluation_results['mean_reward']:.2f} ± {evaluation_results['std_reward']:.2f}\")\n",
    "            print(f\"  Win Rate: {evaluation_results['win_rate']:.2%}\")\n",
    "            print(f\"  Mean Episode Length: {evaluation_results['mean_length']:.1f} ± {evaluation_results['std_length']:.1f}\")\n",
    "            if rsa_results:\n",
    "                print(f\"  RSA Analysis: {len(rsa_results)-3} layers analyzed with {len(logical_states)} states\")\n",
    "            \n",
    "        return {\n",
    "            'evaluation': evaluation_results,\n",
    "            'rsa': rsa_results\n",
    "        }\n",
    "    \n",
    "    def evaluate_all_models_with_rsa(\n",
    "        self,\n",
    "        models: Dict,\n",
    "        num_episodes: int = 10,\n",
    "        max_episode_length: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        max_states_for_rsa: int = 500\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate all models with RSA analysis.\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {len(models)} models on {self.env_name} with RSA analysis\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            try:\n",
    "                results = self.evaluate_model_with_rsa(\n",
    "                    model=model,\n",
    "                    model_name=model_name,\n",
    "                    num_episodes=num_episodes,\n",
    "                    max_episode_length=max_episode_length,\n",
    "                    deterministic=deterministic,\n",
    "                    collect_rsa=True,\n",
    "                    max_states_for_rsa=max_states_for_rsa\n",
    "                )\n",
    "                all_results[model_name] = results\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "                \n",
    "        return all_results\n",
    "    \n",
    "    def compare_rsa_across_models(self):\n",
    "        \"\"\"\n",
    "        Compare RSA results across all evaluated models.\n",
    "        \"\"\"\n",
    "        if not self.rsa_results:\n",
    "            print(\"No RSA results available. Run evaluate_all_models_with_rsa first.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"RSA CORRELATION SUMMARY (ON-POLICY EVALUATION STATES)\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"{'Model':<10} {'Layer':<25} {'N States':<10} {'Pearson':<10} {'Spearman':<10} {'Sim Mean':<10} {'Dissim Mean':<12} {'P-value':<10}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for model_name, rsa_data in self.rsa_results.items():\n",
    "            for layer_name, layer_results in rsa_data.items():\n",
    "                if layer_name in ['behavioral_matrix', 'logical_states', 'pixel_states']:\n",
    "                    continue\n",
    "                    \n",
    "                pearson = layer_results['pearson_corr']\n",
    "                spearman = layer_results['spearman_corr']\n",
    "                n_states = layer_results['n_states']\n",
    "                group_analysis = layer_results['group_analysis']\n",
    "                \n",
    "                sim_mean = group_analysis['similar_mean']\n",
    "                dissim_mean = group_analysis['dissimilar_mean']\n",
    "                p_val = group_analysis.get('p_val', np.nan)\n",
    "                \n",
    "                print(f\"{model_name:<10} {layer_name:<25} {n_states:<10} {pearson:<10.4f} {spearman:<10.4f} {sim_mean:<10.4f} {dissim_mean:<12.4f} {p_val:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef57e0a-f6ab-4ec3-8d4d-df714032394a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation with RSA analysis...\n",
      "Available models: ['ppo', 'a2c']\n",
      "Evaluating 2 models on PongNoFrameskip-v4 with RSA analysis\n",
      "======================================================================\n",
      "Registered hooks for 8 layers\n",
      "\n",
      "Evaluating ppo model with RSA collection...\n",
      "Episodes: 5, Max length: 10000\n",
      "Max states for RSA: 500\n",
      "  Episode 1: Reward=21.0, Length=1667, RSA States=500, Time=1.86s\n",
      "  Episode 2: Reward=21.0, Length=1665, RSA States=500, Time=1.78s\n",
      "  Episode 3: Reward=21.0, Length=1667, RSA States=500, Time=1.78s\n",
      "  Episode 4: Reward=21.0, Length=1668, RSA States=500, Time=1.77s\n",
      "  Episode 5: Reward=21.0, Length=1664, RSA States=500, Time=1.76s\n",
      "\n",
      "Performing RSA analysis with 500 states...\n",
      "Behavioral similarity stats:\n",
      "  total_states: 500\n",
      "  total_pairs: 124750\n",
      "  symmetric_pairs: 409\n",
      "  symmetry_ratio: 0.003278557114228457\n",
      "  diagonal_sum: 500\n",
      "  Processing cnn_layer_0_Conv2d (shape: (500, 32, 20, 20))\n",
      "    Pearson correlation: 0.0041 (p=0.1457)\n",
      "    Spearman correlation: 0.0906 (p=0.0000)\n",
      "    Similar group mean: 0.9958 ± 0.0416\n",
      "    Dissimilar group mean: 0.9868 ± 0.0895\n",
      "    Group difference p-value: 0.0003\n",
      "  Processing cnn_layer_1_ReLU (shape: (500, 32, 20, 20))\n",
      "    Pearson correlation: 0.0296 (p=0.0000)\n",
      "    Spearman correlation: 0.0881 (p=0.0000)\n",
      "    Similar group mean: 0.9810 ± 0.0650\n",
      "    Dissimilar group mean: 0.9222 ± 0.0911\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing cnn_layer_2_Conv2d (shape: (500, 64, 9, 9))\n",
      "    Pearson correlation: 0.0555 (p=0.0000)\n",
      "    Spearman correlation: 0.0899 (p=0.0000)\n",
      "    Similar group mean: 0.9814 ± 0.0450\n",
      "    Dissimilar group mean: 0.9093 ± 0.0625\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing cnn_layer_3_ReLU (shape: (500, 64, 9, 9))\n",
      "    Pearson correlation: 0.1747 (p=0.0000)\n",
      "    Spearman correlation: 0.0909 (p=0.0000)\n",
      "    Similar group mean: 0.9236 ± 0.1135\n",
      "    Dissimilar group mean: 0.6155 ± 0.0840\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing cnn_layer_4_Conv2d (shape: (500, 64, 7, 7))\n",
      "    Pearson correlation: 0.2054 (p=0.0000)\n",
      "    Spearman correlation: 0.0915 (p=0.0000)\n",
      "    Similar group mean: 0.9132 ± 0.1375\n",
      "    Dissimilar group mean: 0.4907 ± 0.1005\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing cnn_layer_5_ReLU (shape: (500, 64, 7, 7))\n",
      "    Pearson correlation: 0.2070 (p=0.0000)\n",
      "    Spearman correlation: 0.0899 (p=0.0000)\n",
      "    Similar group mean: 0.8693 ± 0.2020\n",
      "    Dissimilar group mean: 0.3020 ± 0.1313\n",
      "    Group difference p-value: 0.0000\n",
      "  Processing action_net_Linear (shape: (500, 6))\n",
      "    Pearson correlation: 0.0736 (p=0.0000)\n",
      "    Spearman correlation: 0.0693 (p=0.0000)\n",
      "    Similar group mean: 0.8090 ± 0.3561\n",
      "    Dissimilar group mean: 0.0609 ± 0.4891\n",
      "    Group difference p-value: 0.0000\n",
      "\n",
      "PPO Results:\n",
      "  Mean Reward: 21.00 ± 0.00\n",
      "  Win Rate: 100.00%\n",
      "  Mean Episode Length: 1666.2 ± 1.5\n",
      "  RSA Analysis: 7 layers analyzed with 500 states\n",
      "Registered hooks for 8 layers\n",
      "\n",
      "Evaluating a2c model with RSA collection...\n",
      "Episodes: 5, Max length: 10000\n",
      "Max states for RSA: 500\n",
      "  Episode 1: Reward=19.0, Length=2289, RSA States=500, Time=2.80s\n",
      "  Episode 2: Reward=21.0, Length=1708, RSA States=500, Time=2.14s\n",
      "  Episode 3: Reward=21.0, Length=1704, RSA States=500, Time=2.16s\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced evaluator with RSA capabilities\n",
    "evaluator_rsa = ModelEvaluatorWithRSA(env_name=\"PongNoFrameskip-v4\", render=False)\n",
    "\n",
    "# Evaluate all models with simultaneous RSA collection\n",
    "if 'models' in locals() and models:\n",
    "    print(\"Starting comprehensive evaluation with RSA analysis...\")\n",
    "    print(f\"Available models: {list(models.keys())}\")\n",
    "    \n",
    "    # Run evaluation with RSA collection\n",
    "    # Using fewer episodes but more states per episode for better RSA analysis\n",
    "    comprehensive_results = evaluator_rsa.evaluate_all_models_with_rsa(\n",
    "        models=models,\n",
    "        num_episodes=5,  # Fewer episodes but more states collected per episode\n",
    "        max_episode_length=10000,\n",
    "        deterministic=True,\n",
    "        max_states_for_rsa=500  # Collect up to 500 states for RSA\n",
    "    )\n",
    "    \n",
    "    if comprehensive_results:\n",
    "        print(f\"\\nSuccessfully evaluated {len(comprehensive_results)} models\")\n",
    "        \n",
    "        # Show standard performance comparison\n",
    "        evaluator_rsa.compare_models()\n",
    "        \n",
    "        # Show RSA comparison across models\n",
    "        evaluator_rsa.compare_rsa_across_models()\n",
    "        \n",
    "        # Plot performance (reuse existing method)\n",
    "        evaluator_rsa.plot_performance()\n",
    "    else:\n",
    "        print(\"No models were successfully evaluated.\")\n",
    "else:\n",
    "    print(\"No models available for evaluation. Please run the model download cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db31d5-6456-4db2-b2b1-c556e1dbf250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sac",
   "language": "python",
   "name": "sac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
